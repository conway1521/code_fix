{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEALTHCARE WORKFORCE ANALYSIS TEMPLATE\n",
    "This template provides a standardized framework for analyzing healthcare workforce data across multiple health systems. It emphasizes validation, transparency, and analyst control.\n",
    "\n",
    "**DESIGN PRINCIPLES:**\n",
    "- No silent failures - every step validates its work\n",
    "- User controls all mappings and parameters\n",
    "- Consistent variable naming for downstream use\n",
    "- Heavy validation with hard stops when data quality is insufficient\n",
    "- Modular sections that can be validated step-by-step\n",
    "\n",
    "**WORKFLOW SECTIONS:**\n",
    "1. Data Import & Schema Setup\n",
    "2. Initial Data Exploration & Validation\n",
    "3. Geographic Filtering\n",
    "4. Column Standardization & Mapping\n",
    "5. Job Title Standardization\n",
    "6. Employment Status Standardization\n",
    "7. Location/Facility Standardization & Classification\n",
    "8. Core Metrics Calculation\n",
    "9. Facility-Level Analysis Setup\n",
    "10. Output Generation Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 1: DATA IMPORT & SCHEMA SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from math import gcd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USER CONFIGURATION\n",
    "Modify the settings in the cell below to match your data sources and analysis parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths and data sources\n",
    "DATA_CONFIG = {\n",
    "    'base_folder': '/path/to/your/data',  # Base folder containing data files\n",
    "    'headcount_source': {\n",
    "        'file': 'System2024.xlsx',\n",
    "        'sheet': 'Headcount',\n",
    "        'type': 'excel'  # 'excel' or 'csv'\n",
    "    },\n",
    "    'hires_source': {\n",
    "        'file': 'System2024.xlsx', \n",
    "        'sheet': 'Hires',\n",
    "        'type': 'excel'\n",
    "    },\n",
    "    'terminations_source': {\n",
    "        'file': 'System2024.xlsx',\n",
    "        'sheet': 'Terms', \n",
    "        'type': 'excel'\n",
    "    },\n",
    "    'requisitions_source': {\n",
    "        'file': 'System2024.xlsx',\n",
    "        'sheet': 'Requisitions',\n",
    "        'type': 'excel'\n",
    "    },\n",
    "    'contractors_source': {\n",
    "        'file': 'System2024.xlsx',\n",
    "        'sheet': 'Contractor',\n",
    "        'type': 'excel'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Column mapping - USER MUST SPECIFY THESE\n",
    "COLUMN_MAPPING = {\n",
    "    'job_title_col': 'Job Title',\n",
    "    'location_col': 'Location', \n",
    "    'state_col': 'Work State',\n",
    "    'full_part_time_col': 'Full/Part Time',\n",
    "    'employment_category_col': 'Employment Category',\n",
    "    'hire_date_col': 'Hire Date',  # For hires sheet\n",
    "    'term_date_col': 'Termination Date',  # For terminations sheet\n",
    "    'req_status_col': 'Req Current State',  # For requisitions sheet\n",
    "}\n",
    "\n",
    "# Analysis parameters\n",
    "ANALYSIS_CONFIG = {\n",
    "    'target_state': 'NC',  # State to filter for\n",
    "    'analysis_date': '2024-12-31',  # Date for analysis\n",
    "    'target_job_titles': ['Registered Nurse', 'Licensed Practical Nurse', 'Medical Assistant', 'Certified Nursing Assistant'],\n",
    "    'fte_values': ['FULL_TIME'],  # Values that indicate FTE status\n",
    "    'prn_values': ['PRN'],  # Values that indicate PRN status\n",
    "    'open_req_statuses': ['Not Posted', 'Posted', 'Unposted', 'In Progress'],\n",
    "    'closed_req_statuses': ['Suspended', 'Expired']\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"Target state: {ANALYSIS_CONFIG['target_state']}\")\n",
    "print(f\"Analysis date: {ANALYSIS_CONFIG['analysis_date']}\")\n",
    "print(f\"Target job titles: {ANALYSIS_CONFIG['target_job_titles']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(source_config, dataset_name):\n",
    "    \"\"\"Load a dataset based on configuration\"\"\"\n",
    "    try:\n",
    "        filepath = f\"{DATA_CONFIG['base_folder']}/{source_config['file']}\"\n",
    "        \n",
    "        if source_config['type'] == 'excel':\n",
    "            df = pd.read_excel(filepath, sheet_name=source_config['sheet'])\n",
    "            print(f\"✓ Loaded {dataset_name}: {len(df)} rows from {source_config['file']}/{source_config['sheet']}\")\n",
    "        elif source_config['type'] == 'csv':\n",
    "            df = pd.read_csv(filepath)\n",
    "            print(f\"✓ Loaded {dataset_name}: {len(df)} rows from {source_config['file']}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {source_config['type']}\")\n",
    "            \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ CRITICAL ERROR loading {dataset_name}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Load all datasets\n",
    "print(\"\\nLoading datasets...\")\n",
    "try:\n",
    "    headcount_df = load_dataset(DATA_CONFIG['headcount_source'], 'headcount')\n",
    "    hires_df = load_dataset(DATA_CONFIG['hires_source'], 'hires')\n",
    "    terminations_df = load_dataset(DATA_CONFIG['terminations_source'], 'terminations')\n",
    "    requisitions_df = load_dataset(DATA_CONFIG['requisitions_source'], 'requisitions')\n",
    "    \n",
    "    # Contractors is optional\n",
    "    try:\n",
    "        contractors_df = load_dataset(DATA_CONFIG['contractors_source'], 'contractors')\n",
    "        has_contractors = True\n",
    "    except:\n",
    "        print(\"! No contractors data found - proceeding without it\")\n",
    "        contractors_df = pd.DataFrame()\n",
    "        has_contractors = False\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ CRITICAL: Data loading failed. Cannot proceed.\")\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\n✓ Data loading complete. {4 + int(has_contractors)} datasets loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 2: INITIAL DATA EXPLORATION & VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dataset_structure(df, dataset_name, required_columns):\n",
    "    \"\"\"Validate that dataset has required structure\"\"\"\n",
    "    print(f\"\\nValidating {dataset_name} structure:\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "    \n",
    "    missing_cols = []\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            missing_cols.append(col)\n",
    "            \n",
    "    if missing_cols:\n",
    "        print(f\"✗ CRITICAL: Missing required columns in {dataset_name}: {missing_cols}\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"✓ All required columns present in {dataset_name}\")\n",
    "        return True\n",
    "\n",
    "# Define required columns for each dataset\n",
    "REQUIRED_COLUMNS = {\n",
    "    'headcount': [COLUMN_MAPPING['job_title_col'], COLUMN_MAPPING['location_col'], \n",
    "                  COLUMN_MAPPING['state_col'], COLUMN_MAPPING['full_part_time_col'], \n",
    "                  COLUMN_MAPPING['employment_category_col']],\n",
    "    'hires': [COLUMN_MAPPING['job_title_col'], COLUMN_MAPPING['location_col'], \n",
    "              COLUMN_MAPPING['state_col'], COLUMN_MAPPING['hire_date_col']],\n",
    "    'terminations': [COLUMN_MAPPING['job_title_col'], COLUMN_MAPPING['location_col'], \n",
    "                     COLUMN_MAPPING['state_col'], COLUMN_MAPPING['term_date_col']],\n",
    "    'requisitions': [COLUMN_MAPPING['job_title_col'], COLUMN_MAPPING['location_col'], \n",
    "                     COLUMN_MAPPING['req_status_col']]\n",
    "}\n",
    "\n",
    "# Validate all datasets\n",
    "validation_passed = True\n",
    "datasets_to_validate = [\n",
    "    (headcount_df, 'headcount'),\n",
    "    (hires_df, 'hires'), \n",
    "    (terminations_df, 'terminations'),\n",
    "    (requisitions_df, 'requisitions')\n",
    "]\n",
    "\n",
    "if has_contractors:\n",
    "    datasets_to_validate.append((contractors_df, 'contractors'))\n",
    "\n",
    "for df, name in datasets_to_validate:\n",
    "    if name in REQUIRED_COLUMNS:\n",
    "        if not validate_dataset_structure(df, name, REQUIRED_COLUMNS[name]):\n",
    "            validation_passed = False\n",
    "\n",
    "if not validation_passed:\n",
    "    print(\"\\n✗ CRITICAL: Dataset validation failed. Check column mappings and try again.\")\n",
    "    raise ValueError(\"Dataset validation failed\")\n",
    "\n",
    "print(\"\\n✓ All datasets passed structure validation\")\n",
    "\n",
    "# Show data previews\n",
    "print(\"\\nData previews:\")\n",
    "for df, name in datasets_to_validate:\n",
    "    if len(df) > 0:\n",
    "        print(f\"\\n{name.upper()} - First 3 rows:\")\n",
    "        display(df.head(3))\n",
    "        \n",
    "        # Show unique values for key columns\n",
    "        if COLUMN_MAPPING['state_col'] in df.columns:\n",
    "            states = df[COLUMN_MAPPING['state_col']].unique()\n",
    "            print(f\"  Unique states: {states}\")\n",
    "            \n",
    "        if COLUMN_MAPPING['job_title_col'] in df.columns:\n",
    "            job_titles = df[COLUMN_MAPPING['job_title_col']].value_counts().head(5)\n",
    "            print(f\"  Top 5 job titles: {dict(job_titles)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 3: GEOGRAPHIC FILTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_state(df, dataset_name, state_col, target_state):\n",
    "    \"\"\"Filter dataset by state and validate results\"\"\"\n",
    "    original_count = len(df)\n",
    "    \n",
    "    if state_col not in df.columns:\n",
    "        print(f\"✗ CRITICAL: State column '{state_col}' not found in {dataset_name}\")\n",
    "        raise ValueError(f\"State column missing in {dataset_name}\")\n",
    "    \n",
    "    # Show unique states before filtering\n",
    "    unique_states = df[state_col].unique()\n",
    "    print(f\"\\n{dataset_name} - States before filtering: {unique_states}\")\n",
    "    \n",
    "    # Filter for target state\n",
    "    df_filtered = df[df[state_col] == target_state].copy()\n",
    "    filtered_count = len(df_filtered)\n",
    "    \n",
    "    print(f\"{dataset_name} filtering: {original_count} → {filtered_count} rows\")\n",
    "    \n",
    "    if filtered_count == 0:\n",
    "        print(f\"✗ CRITICAL: No data remains after filtering {dataset_name} for {target_state}\")\n",
    "        raise ValueError(f\"No {target_state} data in {dataset_name}\")\n",
    "    \n",
    "    # Validation - check that filtering worked\n",
    "    remaining_states = df_filtered[state_col].unique()\n",
    "    if len(remaining_states) != 1 or remaining_states[0] != target_state:\n",
    "        print(f\"✗ CRITICAL: Filtering failed for {dataset_name}. Remaining states: {remaining_states}\")\n",
    "        raise ValueError(f\"State filtering failed for {dataset_name}\")\n",
    "    \n",
    "    print(f\"✓ {dataset_name} successfully filtered to {target_state}\")\n",
    "    return df_filtered\n",
    "\n",
    "# Filter all datasets\n",
    "target_state = ANALYSIS_CONFIG['target_state']\n",
    "state_col = COLUMN_MAPPING['state_col']\n",
    "\n",
    "print(f\"Filtering all datasets for state: {target_state}\")\n",
    "\n",
    "headcount_df = filter_by_state(headcount_df, 'headcount', state_col, target_state)\n",
    "hires_df = filter_by_state(hires_df, 'hires', state_col, target_state)\n",
    "terminations_df = filter_by_state(terminations_df, 'terminations', state_col, target_state)\n",
    "requisitions_df = filter_by_state(requisitions_df, 'requisitions', state_col, target_state)\n",
    "\n",
    "if has_contractors:\n",
    "    contractors_df = filter_by_state(contractors_df, 'contractors', state_col, target_state)\n",
    "\n",
    "print(f\"\\n✓ Geographic filtering complete. All datasets filtered to {target_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 4: COLUMN STANDARDIZATION & MAPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_columns(df, dataset_name):\n",
    "    \"\"\"Apply column standardization to ensure consistent naming\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ensure all mapped columns exist and create standard names\n",
    "    column_renames = {}\n",
    "    \n",
    "    # Standard columns all datasets should have\n",
    "    if COLUMN_MAPPING['job_title_col'] in df.columns:\n",
    "        column_renames[COLUMN_MAPPING['job_title_col']] = 'Job_Title'\n",
    "    if COLUMN_MAPPING['location_col'] in df.columns:\n",
    "        column_renames[COLUMN_MAPPING['location_col']] = 'Location'\n",
    "    if COLUMN_MAPPING['state_col'] in df.columns:\n",
    "        column_renames[COLUMN_MAPPING['state_col']] = 'State'\n",
    "        \n",
    "    # Dataset-specific columns\n",
    "    if dataset_name in ['headcount', 'hires', 'terminations']:\n",
    "        if COLUMN_MAPPING['full_part_time_col'] in df.columns:\n",
    "            column_renames[COLUMN_MAPPING['full_part_time_col']] = 'Full_Part_Time'\n",
    "        if COLUMN_MAPPING['employment_category_col'] in df.columns:\n",
    "            column_renames[COLUMN_MAPPING['employment_category_col']] = 'Employment_Category'\n",
    "    \n",
    "    if dataset_name == 'hires' and COLUMN_MAPPING['hire_date_col'] in df.columns:\n",
    "        column_renames[COLUMN_MAPPING['hire_date_col']] = 'Date'\n",
    "    if dataset_name == 'terminations' and COLUMN_MAPPING['term_date_col'] in df.columns:\n",
    "        column_renames[COLUMN_MAPPING['term_date_col']] = 'Date'\n",
    "    if dataset_name == 'requisitions' and COLUMN_MAPPING['req_status_col'] in df.columns:\n",
    "        column_renames[COLUMN_MAPPING['req_status_col']] = 'Req_Status'\n",
    "    \n",
    "    # Apply renames\n",
    "    df = df.rename(columns=column_renames)\n",
    "    \n",
    "    print(f\"✓ {dataset_name} columns standardized. Renamed: {column_renames}\")\n",
    "    return df\n",
    "\n",
    "# Standardize all datasets\n",
    "print(\"Standardizing column names across all datasets...\")\n",
    "\n",
    "headcount_df = standardize_columns(headcount_df, 'headcount')\n",
    "hires_df = standardize_columns(hires_df, 'hires')\n",
    "terminations_df = standardize_columns(terminations_df, 'terminations')\n",
    "requisitions_df = standardize_columns(requisitions_df, 'requisitions')\n",
    "\n",
    "if has_contractors:\n",
    "    contractors_df = standardize_columns(contractors_df, 'contractors')\n",
    "\n",
    "print(\"\\n✓ All datasets have standardized column names\")\n",
    "\n",
    "# Validation - ensure key columns are present\n",
    "for df, name in [(headcount_df, 'headcount'), (hires_df, 'hires'), \n",
    "                 (terminations_df, 'terminations'), (requisitions_df, 'requisitions')]:\n",
    "    if 'Job_Title' not in df.columns or 'Location' not in df.columns:\n",
    "        print(f\"✗ CRITICAL: Missing standardized columns in {name}\")\n",
    "        raise ValueError(f\"Column standardization failed for {name}\")\n",
    "\n",
    "print(\"\\n✓ Column standardization validation passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 5: JOB TITLE STANDARDIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_job_title(title):\n",
    "    \"\"\"\n",
    "    Standardize job titles to target categories:\n",
    "    - Registered Nurse\n",
    "    - Licensed Practical Nurse  \n",
    "    - Medical Assistant\n",
    "    - Certified Nursing Assistant\n",
    "    \"\"\"\n",
    "    if pd.isna(title):\n",
    "        return None\n",
    "    \n",
    "    title_lower = str(title).lower().strip()\n",
    "    \n",
    "    # Registered Nurse variations\n",
    "    if 'rn' in title_lower or 'registered nurse' in title_lower:\n",
    "        # Exclude non-employee entries\n",
    "        if '(nonee)' in title_lower or 'traveler' in title_lower:\n",
    "            return None\n",
    "        return 'Registered Nurse'\n",
    "    \n",
    "    # Licensed Practical Nurse variations\n",
    "    elif 'lpn' in title_lower or 'licensed practical nurse' in title_lower:\n",
    "        return 'Licensed Practical Nurse'\n",
    "    \n",
    "    # Medical Assistant variations\n",
    "    elif 'medical assistant' in title_lower or 'certified medical assistant' in title_lower:\n",
    "        return 'Medical Assistant'\n",
    "    \n",
    "    # Certified Nursing Assistant variations\n",
    "    elif ('cna' in title_lower or 'certified nursing assistant' in title_lower or \n",
    "          'certified nurse assistant' in title_lower or 'nurse assistant' in title_lower or \n",
    "          'nursing assistant' in title_lower):\n",
    "        return 'Certified Nursing Assistant'\n",
    "    \n",
    "    # If none match, return None (will be filtered out)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def apply_job_title_standardization(df, dataset_name):\n",
    "    \"\"\"Apply job title standardization and validate results\"\"\"\n",
    "    original_count = len(df)\n",
    "    \n",
    "    # Apply standardization\n",
    "    df = df.copy()\n",
    "    df['Standardized_Job_Title'] = df['Job_Title'].apply(standardize_job_title)\n",
    "    \n",
    "    # Filter for target roles only\n",
    "    df_filtered = df[df['Standardized_Job_Title'].notna()].copy()\n",
    "    filtered_count = len(df_filtered)\n",
    "    \n",
    "    # Calculate capture rate\n",
    "    capture_rate = (filtered_count / original_count * 100) if original_count > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{dataset_name} job title standardization:\")\n",
    "    print(f\"  Original rows: {original_count}\")\n",
    "    print(f\"  Rows with target job titles: {filtered_count}\")\n",
    "    print(f\"  Capture rate: {capture_rate:.1f}%\")\n",
    "    \n",
    "    # Show distribution of standardized titles\n",
    "    title_dist = df_filtered['Standardized_Job_Title'].value_counts()\n",
    "    print(f\"  Distribution: {dict(title_dist)}\")\n",
    "    \n",
    "    # Validation - ensure we have reasonable capture rate\n",
    "    if capture_rate < 10:\n",
    "        print(f\"✗ CRITICAL: Very low capture rate ({capture_rate:.1f}%) for {dataset_name}\")\n",
    "        print(\"This suggests job title standardization is not working properly.\")\n",
    "        # Show some examples of unmapped titles\n",
    "        unmapped = df[df['Standardized_Job_Title'].isna()]['Job_Title'].value_counts().head(10)\n",
    "        print(f\"Top unmapped titles: {dict(unmapped)}\")\n",
    "        raise ValueError(f\"Job title standardization failed for {dataset_name}\")\n",
    "    \n",
    "    if filtered_count == 0:\n",
    "        print(f\"✗ CRITICAL: No target job titles found in {dataset_name}\")\n",
    "        raise ValueError(f\"No target job titles in {dataset_name}\")\n",
    "    \n",
    "    print(f\"✓ {dataset_name} job title standardization successful\")\n",
    "    return df_filtered\n",
    "\n",
    "# Apply job title standardization to all datasets\n",
    "print(\"Applying job title standardization...\")\n",
    "print(f\"Target job titles: {ANALYSIS_CONFIG['target_job_titles']}\")\n",
    "\n",
    "headcount_filtered = apply_job_title_standardization(headcount_df, 'headcount')\n",
    "hires_filtered = apply_job_title_standardization(hires_df, 'hires')\n",
    "terminations_filtered = apply_job_title_standardization(terminations_df, 'terminations')\n",
    "requisitions_filtered = apply_job_title_standardization(requisitions_df, 'requisitions')\n",
    "\n",
    "if has_contractors:\n",
    "    contractors_filtered = apply_job_title_standardization(contractors_df, 'contractors')\n",
    "\n",
    "print(\"\\n✓ Job title standardization complete for all datasets\")\n",
    "\n",
    "# Cross-dataset validation - ensure we have consistent job titles\n",
    "all_job_titles = set()\n",
    "for df, name in [(headcount_filtered, 'headcount'), (hires_filtered, 'hires'),\n",
    "                 (terminations_filtered, 'terminations'), (requisitions_filtered, 'requisitions')]:\n",
    "    titles = set(df['Standardized_Job_Title'].unique())\n",
    "    all_job_titles.update(titles)\n",
    "    \n",
    "expected_titles = set(ANALYSIS_CONFIG['target_job_titles'])\n",
    "if not all_job_titles.issubset(expected_titles):\n",
    "    print(f\"✗ WARNING: Unexpected job titles found: {all_job_titles - expected_titles}\")\n",
    "\n",
    "print(f\"✓ Job title validation complete. Found titles: {sorted(all_job_titles)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 6: EMPLOYMENT STATUS STANDARDIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_standardized_employment_status(df, dataset_name):\n",
    "    \"\"\"Create standardized FTE/PRN employment status\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Check required columns exist\n",
    "    if 'Full_Part_Time' not in df.columns or 'Employment_Category' not in df.columns:\n",
    "        print(f\"✗ CRITICAL: Missing employment status columns in {dataset_name}\")\n",
    "        raise ValueError(f\"Employment status columns missing in {dataset_name}\")\n",
    "    \n",
    "    # Show original distributions\n",
    "    print(f\"\\n{dataset_name} employment status standardization:\")\n",
    "    ft_dist = df['Full_Part_Time'].value_counts()\n",
    "    emp_dist = df['Employment_Category'].value_counts()\n",
    "    print(f\"  Full_Part_Time distribution: {dict(ft_dist)}\")\n",
    "    print(f\"  Employment_Category distribution: {dict(emp_dist)}\")\n",
    "    \n",
    "    # Create standardized status\n",
    "    df['Standardized_Employment_Status'] = ''\n",
    "    \n",
    "    # Set FTE for full-time employees\n",
    "    fte_values = ANALYSIS_CONFIG['fte_values']\n",
    "    fte_mask = df['Full_Part_Time'].isin(fte_values)\n",
    "    df.loc[fte_mask, 'Standardized_Employment_Status'] = 'FTE'\n",
    "    \n",
    "    # Override with PRN where applicable  \n",
    "    prn_values = ANALYSIS_CONFIG['prn_values']\n",
    "    prn_mask = df['Employment_Category'].isin(prn_values)\n",
    "    df.loc[prn_mask, 'Standardized_Employment_Status'] = 'PRN'\n",
    "    \n",
    "    # Validation\n",
    "    fte_count = (df['Standardized_Employment_Status'] == 'FTE').sum()\n",
    "    prn_count = (df['Standardized_Employment_Status'] == 'PRN').sum()\n",
    "    other_count = len(df) - fte_count - prn_count\n",
    "    \n",
    "    print(f\"  Standardized employment status:\")\n",
    "    print(f\"    FTE: {fte_count}\")\n",
    "    print(f\"    PRN: {prn_count}\")\n",
    "    print(f\"    Other/Missing: {other_count}\")\n",
    "    \n",
    "    if other_count > 0:\n",
    "        print(f\"✗ WARNING: {other_count} records with unclear employment status in {dataset_name}\")\n",
    "        # Show examples\n",
    "        other_examples = df[~df['Standardized_Employment_Status'].isin(['FTE', 'PRN'])][\n",
    "            ['Full_Part_Time', 'Employment_Category']].drop_duplicates().head(5)\n",
    "        print(f\"Examples of unclear status:\\n{other_examples}\")\n",
    "    \n",
    "    if fte_count == 0 and prn_count == 0:\n",
    "        print(f\"✗ CRITICAL: No valid employment status found in {dataset_name}\")\n",
    "        raise ValueError(f\"Employment status standardization failed for {dataset_name}\")\n",
    "    \n",
    "    print(f\"✓ {dataset_name} employment status standardization complete\")\n",
    "    return df\n",
    "\n",
    "# Apply employment status standardization\n",
    "print(\"Creating standardized employment status...\")\n",
    "print(f\"FTE values: {ANALYSIS_CONFIG['fte_values']}\")\n",
    "print(f\"PRN values: {ANALYSIS_CONFIG['prn_values']}\")\n",
    "\n",
    "headcount_filtered = create_standardized_employment_status(headcount_filtered, 'headcount')\n",
    "hires_filtered = create_standardized_employment_status(hires_filtered, 'hires')\n",
    "terminations_filtered = create_standardized_employment_status(terminations_filtered, 'terminations')\n",
    "\n",
    "if has_contractors:\n",
    "    contractors_filtered = create_standardized_employment_status(contractors_filtered, 'contractors')\n",
    "\n",
    "# Create FTE and PRN subsets for analysis\n",
    "print(\"\\nCreating employment status subsets...\")\n",
    "\n",
    "# Headcount subsets\n",
    "headcount_filtered_fte = headcount_filtered[headcount_filtered['Standardized_Employment_Status'] == 'FTE'].copy()\n",
    "headcount_filtered_prn = headcount_filtered[headcount_filtered['Standardized_Employment_Status'] == 'PRN'].copy()\n",
    "\n",
    "# Hires subsets  \n",
    "hires_filtered_fte = hires_filtered[hires_filtered['Standardized_Employment_Status'] == 'FTE'].copy()\n",
    "hires_filtered_prn = hires_filtered[hires_filtered['Standardized_Employment_Status'] == 'PRN'].copy()\n",
    "\n",
    "# Terminations subsets\n",
    "terminations_filtered_fte = terminations_filtered[terminations_filtered['Standardized_Employment_Status'] == 'FTE'].copy()\n",
    "terminations_filtered_prn = terminations_filtered[terminations_filtered['Standardized_Employment_Status'] == 'PRN'].copy()\n",
    "\n",
    "print(\"Employment status subsets created:\")\n",
    "print(f\"  Headcount - FTE: {len(headcount_filtered_fte)}, PRN: {len(headcount_filtered_prn)}\")\n",
    "print(f\"  Hires - FTE: {len(hires_filtered_fte)}, PRN: {len(hires_filtered_prn)}\")\n",
    "print(f\"  Terminations - FTE: {len(terminations_filtered_fte)}, PRN: {len(terminations_filtered_prn)}\")\n",
    "\n",
    "print(\"\\n✓ Employment status standardization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 7: LOCATION/FACILITY STANDARDIZATION & CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_location(location):\n",
    "    \"\"\"Clean and standardize location names\"\"\"\n",
    "    if pd.isna(location):\n",
    "        return location\n",
    "    \n",
    "    # Convert to string and basic cleaning\n",
    "    loc = str(location).strip()\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    loc = re.sub(r'\\s+', ' ', loc)\n",
    "    \n",
    "    # Remove trailing punctuation and symbols\n",
    "    loc = re.sub(r'[,.\\-\\s]+$",
    " '', loc)\n",
    "    \n",
    "    # Standardize common abbreviations\n",
    "    loc = re.sub(r'\\bSt\\b', 'St', loc)  # Standardize Street\n",
    "    loc = re.sub(r'\\bDr\\b', 'Dr', loc)  # Standardize Drive\n",
    "    loc = re.sub(r'\\bSte\\b', 'Suite', loc)  # Standardize Suite\n",
    "    \n",
    "    # Fix state abbreviations (add comma before state)\n",
    "    loc = re.sub(r'\\s+(NC|SC|VA|TN|GA)\\s*$",
    " r', \\1', loc)\n",
    "    \n",
    "    return loc.strip()\n",
    "\n",
    "def classify_facility_type_detailed(facility_name):\n",
    "    \"\"\"\n",
    "    Classify facility type and sub-type based on keywords in facility name\n",
    "    Returns tuple: (primary_type, sub_type)\n",
    "    \"\"\"\n",
    "    if pd.isna(facility_name):\n",
    "        return 'Unknown', 'Unknown'\n",
    "    \n",
    "    name_lower = str(facility_name).lower()\n",
    "    \n",
    "    # Hospital classification\n",
    "    if any(keyword in name_lower for keyword in [\n",
    "        'hospital', 'medical center', 'medical centre', 'regional medical', \n",
    "        'health system', 'emergency', 'trauma', 'intensive care', 'icu',\n",
    "        'surgery center', 'surgical center', 'ambulatory surgery'\n",
    "    ]):\n",
    "        # Sub-type classification for hospitals\n",
    "        if any(keyword in name_lower for keyword in ['children', 'pediatric', 'peds']):\n",
    "            sub_type = 'Pediatric Hospital'\n",
    "        elif any(keyword in name_lower for keyword in ['regional', 'main', 'flagship']):\n",
    "            sub_type = 'Regional Hospital'\n",
    "        elif any(keyword in name_lower for keyword in ['community', 'local']):\n",
    "            sub_type = 'Community Hospital'\n",
    "        elif any(keyword in name_lower for keyword in ['specialty', 'surgical', 'surgery']):\n",
    "            sub_type = 'Specialty Hospital'\n",
    "        else:\n",
    "            sub_type = 'General Hospital'\n",
    "        \n",
    "        return 'Hospital', sub_type\n",
    "    \n",
    "    # Clinic classification\n",
    "    elif any(keyword in name_lower for keyword in [\n",
    "        'clinic', 'medical group', 'family medicine', 'primary care', 'urgent care',\n",
    "        'outpatient', 'ambulatory', 'practice', 'office', 'center', 'specialty',\n",
    "        'cardiology', 'orthopedic', 'oncology', 'cancer', 'heart', 'diabetes',\n",
    "        'rehabilitation', 'physical therapy', 'occupational health', 'employee health',\n",
    "        'fracture', 'ent', 'bariatrics', 'imaging', 'gynecology', 'endoscopy',\n",
    "        'physician', 'pulmonary', 'endocrine', 'surgery', 'surg', 'weight', 'pain',\n",
    "        'walk-in', 'womancare', 'pulmonology', 'rheumatology', 'arthritis', 'spine',\n",
    "        'vascular', 'hyperbarics', 'healthcare', 'critical care', 'infectious',\n",
    "        'neuroscience', 'women\\'s', 'sports', 'mobile', 'neuro'\n",
    "    ]):\n",
    "        # Sub-type classification for clinics\n",
    "        if any(keyword in name_lower for keyword in ['dental', 'dentist']):\n",
    "            sub_type = 'Dental'\n",
    "        elif any(keyword in name_lower for keyword in ['eye', 'vision', 'ophthalmology', 'optometry']):\n",
    "            sub_type = 'Eye Care'\n",
    "        elif any(keyword in name_lower for keyword in ['urgent care', 'walk-in']):\n",
    "            sub_type = 'Urgent Care'\n",
    "        elif any(keyword in name_lower for keyword in ['primary care', 'family medicine']):\n",
    "            sub_type = 'Primary Care'\n",
    "        else:\n",
    "            sub_type = 'Specialty Clinic'\n",
    "        \n",
    "        return 'Clinic', sub_type\n",
    "    \n",
    "    # Default fallback\n",
    "    else:\n",
    "        return 'Other', 'Other'\n",
    "\n",
    "def apply_location_standardization(datasets_dict):\n",
    "    \"\"\"Apply location standardization and facility classification to all datasets\"\"\"\n",
    "    print(\"Standardizing locations across all datasets...\")\n",
    "    \n",
    "    # Collect all unique locations\n",
    "    all_locations = set()\n",
    "    for name, df in datasets_dict.items():\n",
    "        if 'Location' in df.columns:\n",
    "            locations = df['Location'].dropna().unique()\n",
    "            all_locations.update(locations)\n",
    "    \n",
    "    original_location_count = len(all_locations)\n",
    "    print(f\"Found {original_location_count} unique locations across all datasets\")\n",
    "    \n",
    "    # Apply location standardization to each dataset\n",
    "    for name, df in datasets_dict.items():\n",
    "        if 'Location' in df.columns:\n",
    "            print(f\"  Standardizing locations in {name}...\")\n",
    "            df['Location'] = df['Location'].apply(standardize_location)\n",
    "    \n",
    "    # Re-collect locations after standardization\n",
    "    all_locations_clean = set()\n",
    "    for name, df in datasets_dict.items():\n",
    "        if 'Location' in df.columns:\n",
    "            locations = df['Location'].dropna().unique()\n",
    "            all_locations_clean.update(locations)\n",
    "    \n",
    "    cleaned_location_count = len(all_locations_clean)\n",
    "    print(f\"After standardization: {cleaned_location_count} unique locations\")\n",
    "    print(f\"Consolidation: {original_location_count - cleaned_location_count} duplicates removed\")\n",
    "    \n",
    "    # Apply facility type classification\n",
    "    print(\"\\nApplying facility type classification...\")\n",
    "    \n",
    "    # Create facility type lookup\n",
    "    unique_facilities = pd.DataFrame({'Facility_Name': sorted(all_locations_clean)})\n",
    "    facility_classifications = unique_facilities['Facility_Name'].apply(classify_facility_type_detailed)\n",
    "    unique_facilities['Facility_Type'] = [classification[0] for classification in facility_classifications]\n",
    "    unique_facilities['Facility_Sub_Type'] = [classification[1] for classification in facility_classifications]\n",
    "    \n",
    "    # Show facility type distribution\n",
    "    print(\"\\nFacility type distribution:\")\n",
    "    type_dist = unique_facilities['Facility_Type'].value_counts()\n",
    "    for facility_type, count in type_dist.items():\n",
    "        print(f\"  {facility_type}: {count} facilities\")\n",
    "        \n",
    "        # Show sub-types for each primary type\n",
    "        sub_types = unique_facilities[unique_facilities['Facility_Type'] == facility_type]['Facility_Sub_Type'].value_counts()\n",
    "        for sub_type, sub_count in sub_types.items():\n",
    "            print(f\"    {sub_type}: {sub_count}\")\n",
    "    \n",
    "    # Add facility types to all datasets\n",
    "    facility_lookup = unique_facilities.set_index('Facility_Name')[['Facility_Type', 'Facility_Sub_Type']]\n",
    "    \n",
    "    for name, df in datasets_dict.items():\n",
    "        if 'Location' in df.columns:\n",
    "            print(f\"  Adding facility types to {name}...\")\n",
    "            df['Facility_Type'] = df['Location'].map(facility_lookup['Facility_Type'])\n",
    "            df['Facility_Sub_Type'] = df['Location'].map(facility_lookup['Facility_Sub_Type'])\n",
    "            \n",
    "            # Validation - ensure all locations got classified\n",
    "            unclassified = df['Facility_Type'].isna().sum()\n",
    "            if unclassified > 0:\n",
    "                print(f\"✗ WARNING: {unclassified} locations in {name} could not be classified\")\n",
    "                unclassified_locs = df[df['Facility_Type'].isna()]['Location'].unique()[:5]\n",
    "                print(f\"Examples: {list(unclassified_locs)}\")\n",
    "    \n",
    "    print(\"✓ Location standardization and facility classification complete\")\n",
    "    return datasets_dict\n",
    "\n",
    "# Apply location standardization\n",
    "datasets_for_location = {\n",
    "    'headcount_filtered': headcount_filtered,\n",
    "    'hires_filtered': hires_filtered,\n",
    "    'terminations_filtered': terminations_filtered,\n",
    "    'requisitions_filtered': requisitions_filtered\n",
    "}\n",
    "\n",
    "if has_contractors:\n",
    "    datasets_for_location['contractors_filtered'] = contractors_filtered\n",
    "\n",
    "datasets_for_location = apply_location_standardization(datasets_for_location)\n",
    "\n",
    "# Update dataset variables\n",
    "headcount_filtered = datasets_for_location['headcount_filtered']\n",
    "hires_filtered = datasets_for_location['hires_filtered']\n",
    "terminations_filtered = datasets_for_location['terminations_filtered']\n",
    "requisitions_filtered = datasets_for_location['requisitions_filtered']\n",
    "\n",
    "if has_contractors:\n",
    "    contractors_filtered = datasets_for_location['contractors_filtered']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 8: CORE METRICS CALCULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_system_wide_metrics():\n",
    "    \"\"\"Calculate system-wide workforce metrics\"\"\"\n",
    "    print(\"Calculating system-wide metrics...\")\n",
    "    \n",
    "    # Basic counts (FTE only for primary metrics)\n",
    "    total_headcount_fte = len(headcount_filtered_fte)\n",
    "    total_hires_fte = len(hires_filtered_fte)\n",
    "    total_terminations_fte = len(terminations_filtered_fte)\n",
    "    \n",
    "    # Contingent workforce counts\n",
    "    total_headcount_prn = len(headcount_filtered_prn)\n",
    "    total_hires_prn = len(hires_filtered_prn)\n",
    "    total_terminations_prn = len(terminations_filtered_prn)\n",
    "    \n",
    "    # All workforce counts\n",
    "    total_headcount_all = len(headcount_filtered)\n",
    "    total_hires_all = len(hires_filtered)\n",
    "    total_terminations_all = len(terminations_filtered)\n",
    "    \n",
    "    # Facility counts\n",
    "    all_facilities = pd.concat([\n",
    "        headcount_filtered['Location'],\n",
    "        hires_filtered['Location'], \n",
    "        terminations_filtered['Location']\n",
    "    ]).drop_duplicates()\n",
    "    num_facilities = all_facilities.nunique()\n",
    "    \n",
    "    # Requisitions analysis\n",
    "    open_requisitions = 0\n",
    "    closed_requisitions = 0\n",
    "    total_requisitions = len(requisitions_filtered)\n",
    "    \n",
    "    if 'Req_Status' in requisitions_filtered.columns:\n",
    "        open_statuses = ANALYSIS_CONFIG['open_req_statuses']\n",
    "        closed_statuses = ANALYSIS_CONFIG['closed_req_statuses']\n",
    "        \n",
    "        open_requisitions = requisitions_filtered['Req_Status'].isin(open_statuses).sum()\n",
    "        closed_requisitions = requisitions_filtered['Req_Status'].isin(closed_statuses).sum()\n",
    "    \n",
    "    # Calculate derived metrics\n",
    "    net_change_fte = total_hires_fte - total_terminations_fte\n",
    "    churn_rate_fte = (total_terminations_fte / total_headcount_fte * 100) if total_headcount_fte > 0 else 0\n",
    "    vacancy_rate = (open_requisitions / total_headcount_all * 100) if total_headcount_all > 0 else 0\n",
    "    \n",
    "    # Contingent to FTE ratio\n",
    "    if total_headcount_fte > 0:\n",
    "        ratio_prn = total_headcount_prn\n",
    "        ratio_fte = total_headcount_fte\n",
    "        common_divisor = gcd(ratio_prn, ratio_fte) if ratio_prn > 0 else 1\n",
    "        ratio_prn_simplified = ratio_prn // common_divisor if ratio_prn > 0 else 0\n",
    "        ratio_fte_simplified = ratio_fte // common_divisor\n",
    "        contingent_to_fte_ratio = f\"{ratio_prn_simplified}:{ratio_fte_simplified}\"\n",
    "    else:\n",
    "        contingent_to_fte_ratio = \"N/A\"\n",
    "    \n",
    "    # Create system overview\n",
    "    system_overview = {\n",
    "        'Analysis_Date': ANALYSIS_CONFIG['analysis_date'],\n",
    "        'Target_State': ANALYSIS_CONFIG['target_state'],\n",
    "        'Total_Headcount_FTE': total_headcount_fte,\n",
    "        'Total_Headcount_PRN': total_headcount_prn,\n",
    "        'Total_Headcount_All': total_headcount_all,\n",
    "        'Total_Hires_FTE': total_hires_fte,\n",
    "        'Total_Hires_PRN': total_hires_prn,\n",
    "        'Total_Hires_All': total_hires_all,\n",
    "        'Total_Terminations_FTE': total_terminations_fte,\n",
    "        'Total_Terminations_PRN': total_terminations_prn,\n",
    "        'Total_Terminations_All': total_terminations_all,\n",
    "        'Net_Change_FTE': net_change_fte,\n",
    "        'Open_Requisitions': open_requisitions,\n",
    "        'Closed_Requisitions': closed_requisitions,\n",
    "        'Total_Requisitions': total_requisitions,\n",
    "        'Vacancy_Rate_Percent': round(vacancy_rate, 1),\n",
    "        'Churn_Rate_FTE_Percent': round(churn_rate_fte, 1),\n",
    "        'Contingent_to_FTE_Ratio': contingent_to_fte_ratio,\n",
    "        'Number_of_Facilities': num_facilities\n",
    "    }\n",
    "    \n",
    "    print(\"SYSTEM-WIDE METRICS:\")\n",
    "    for key, value in system_overview.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Validation checks\n",
    "    print(f\"\\nVALIDATION CHECKS:\")\n",
    "    print(f\"  Total headcount matches: {total_headcount_fte + total_headcount_prn == total_headcount_all}\")\n",
    "    print(f\"  Total hires matches: {total_hires_fte + total_hires_prn == total_hires_all}\")\n",
    "    print(f\"  Total terminations matches: {total_terminations_fte + total_terminations_prn == total_terminations_all}\")\n",
    "    print(f\"  Requisitions categorized: {open_requisitions + closed_requisitions} of {total_requisitions}\")\n",
    "    \n",
    "    return system_overview\n",
    "\n",
    "system_overview = calculate_system_wide_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_job_title_metrics():\n",
    "    \"\"\"Calculate metrics by standardized job title\"\"\"\n",
    "    print(\"\\nCalculating metrics by job title...\")\n",
    "    \n",
    "    # Headcount by job title\n",
    "    headcount_by_role = headcount_filtered.groupby('Standardized_Job_Title').size().rename('Headcount_All')\n",
    "    headcount_fte_by_role = headcount_filtered_fte.groupby('Standardized_Job_Title').size().rename('Headcount_FTE')\n",
    "    headcount_prn_by_role = headcount_filtered_prn.groupby('Standardized_Job_Title').size().rename('Headcount_PRN')\n",
    "    \n",
    "    # Hires by job title\n",
    "    hires_by_role = hires_filtered.groupby('Standardized_Job_Title').size().rename('Hires_All')\n",
    "    hires_fte_by_role = hires_filtered_fte.groupby('Standardized_Job_Title').size().rename('Hires_FTE')\n",
    "    hires_prn_by_role = hires_filtered_prn.groupby('Standardized_Job_Title').size().rename('Hires_PRN')\n",
    "    \n",
    "    # Terminations by job title\n",
    "    terminations_by_role = terminations_filtered.groupby('Standardized_Job_Title').size().rename('Terminations_All')\n",
    "    terminations_fte_by_role = terminations_filtered_fte.groupby('Standardized_Job_Title').size().rename('Terminations_FTE')\n",
    "    terminations_prn_by_role = terminations_filtered_prn.groupby('Standardized_Job_Title').size().rename('Terminations_PRN')\n",
    "    \n",
    "    # Requisitions by job title\n",
    "    if 'Req_Status' in requisitions_filtered.columns:\n",
    "        open_statuses = ANALYSIS_CONFIG['open_req_statuses']\n",
    "        closed_statuses = ANALYSIS_CONFIG['closed_req_statuses']\n",
    "        \n",
    "        requisitions_by_role = requisitions_filtered.groupby('Standardized_Job_Title').agg(\n",
    "            Open_Requisitions=('Req_Status', lambda x: x.isin(open_statuses).sum()),\n",
    "            Closed_Requisitions=('Req_Status', lambda x: x.isin(closed_statuses).sum()),\n",
    "            Total_Requisitions=('Req_Status', 'count')\n",
    "        )\n",
    "    else:\n",
    "        # If no status column, just count total requisitions\n",
    "        requisitions_by_role = requisitions_filtered.groupby('Standardized_Job_Title').size().to_frame('Total_Requisitions')\n",
    "        requisitions_by_role['Open_Requisitions'] = 0\n",
    "        requisitions_by_role['Closed_Requisitions'] = 0\n",
    "    \n",
    "    # Facilities by job title\n",
    "    facilities_by_role = headcount_filtered.groupby('Standardized_Job_Title')['Location'].nunique().rename('Number_of_Facilities')\n",
    "    \n",
    "    # Combine all metrics\n",
    "    job_title_summary = pd.DataFrame({\n",
    "        'Headcount_All': headcount_by_role,\n",
    "        'Headcount_FTE': headcount_fte_by_role,\n",
    "        'Headcount_PRN': headcount_prn_by_role,\n",
    "        'Hires_All': hires_by_role,\n",
    "        'Hires_FTE': hires_fte_by_role,\n",
    "        'Hires_PRN': hires_prn_by_role,\n",
    "        'Terminations_All': terminations_by_role,\n",
    "        'Terminations_FTE': terminations_fte_by_role,\n",
    "        'Terminations_PRN': terminations_prn_by_role,\n",
    "        'Open_Requisitions': requisitions_by_role['Open_Requisitions'],\n",
    "        'Closed_Requisitions': requisitions_by_role['Closed_Requisitions'],\n",
    "        'Total_Requisitions': requisitions_by_role['Total_Requisitions'],\n",
    "        'Number_of_Facilities': facilities_by_role\n",
    "    }).fillna(0)\n",
    "    \n",
    "    # Calculate derived metrics\n",
    "    job_title_summary['Net_Change_FTE'] = job_title_summary['Hires_FTE'] - job_title_summary['Terminations_FTE']\n",
    "    job_title_summary['Turnover_Rate_FTE_Percent'] = (\n",
    "        job_title_summary['Terminations_FTE'] / job_title_summary['Headcount_FTE'] * 100\n",
    "    ).round(1)\n",
    "    job_title_summary['Vacancy_Rate_Percent'] = (\n",
    "        job_title_summary['Open_Requisitions'] / job_title_summary['Headcount_All'] * 100\n",
    "    ).round(1)\n",
    "    \n",
    "    # Calculate contingent ratios\n",
    "    def calculate_contingent_ratio(row):\n",
    "        prn_count = int(row['Headcount_PRN'])\n",
    "        fte_count = int(row['Headcount_FTE'])\n",
    "        \n",
    "        if fte_count == 0 and prn_count > 0:\n",
    "            return f\"{prn_count}:0\"\n",
    "        elif prn_count == 0:\n",
    "            return \"0:1\"\n",
    "        else:\n",
    "            common_divisor = gcd(prn_count, fte_count)\n",
    "            prn_simplified = prn_count // common_divisor\n",
    "            fte_simplified = fte_count // common_divisor\n",
    "            return f\"{prn_simplified}:{fte_simplified}\"\n",
    "    \n",
    "    job_title_summary['Contingent_to_FTE_Ratio'] = job_title_summary.apply(calculate_contingent_ratio, axis=1)\n",
    "    \n",
    "    print(\"JOB TITLE METRICS:\")\n",
    "    display(job_title_summary)\n",
    "    \n",
    "    # Validation - check totals match system-wide\n",
    "    total_headcount_check = job_title_summary['Headcount_All'].sum()\n",
    "    total_hires_check = job_title_summary['Hires_All'].sum()\n",
    "    total_terms_check = job_title_summary['Terminations_All'].sum()\n",
    "    \n",
    "    print(f\"\\nVALIDATION - Job title totals vs system totals:\")\n",
    "    print(f\"  Headcount: {total_headcount_check} (should equal {len(headcount_filtered)})\")\n",
    "    print(f\"  Hires: {total_hires_check} (should equal {len(hires_filtered)})\")\n",
    "    print(f\"  Terminations: {total_terms_check} (should equal {len(terminations_filtered)})\")\n",
    "    \n",
    "    if (total_headcount_check != len(headcount_filtered) or \n",
    "        total_hires_check != len(hires_filtered) or \n",
    "        total_terms_check != len(terminations_filtered)):\n",
    "        print(\"✗ CRITICAL: Job title totals don't match system totals!\")\n",
    "        raise ValueError(\"Job title aggregation failed validation\")\n",
    "    \n",
    "    print(\"✓ Job title metrics validation passed\")\n",
    "    return job_title_summary\n",
    "\n",
    "job_title_summary = calculate_job_title_metrics()\n",
    "print(\"\\n✓ Core metrics calculation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 9: FACILITY-LEVEL ANALYSIS SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_facility_level_metrics():\n",
    "    \"\"\"Create comprehensive facility-level workforce metrics\"\"\"\n",
    "    print(\"Creating facility-level metrics by job title...\")\n",
    "    \n",
    "    # Calculate metrics by facility and job title\n",
    "    headcount_by_facility_role = headcount_filtered.groupby(['Location', 'Standardized_Job_Title']).size().rename('Headcount_All')\n",
    "    headcount_fte_by_facility_role = headcount_filtered_fte.groupby(['Location', 'Standardized_Job_Title']).size().rename('Headcount_FTE')\n",
    "    headcount_prn_by_facility_role = headcount_filtered_prn.groupby(['Location', 'Standardized_Job_Title']).size().rename('Headcount_PRN')\n",
    "    \n",
    "    hires_by_facility_role = hires_filtered.groupby(['Location', 'Standardized_Job_Title']).size().rename('Hires_All')\n",
    "    hires_fte_by_facility_role = hires_filtered_fte.groupby(['Location', 'Standardized_Job_Title']).size().rename('Hires_FTE')\n",
    "    hires_prn_by_facility_role = hires_filtered_prn.groupby(['Location', 'Standardized_Job_Title']).size().rename('Hires_PRN')\n",
    "    \n",
    "    terminations_by_facility_role = terminations_filtered.groupby(['Location', 'Standardized_Job_Title']).size().rename('Terminations_All')\n",
    "    terminations_fte_by_facility_role = terminations_filtered_fte.groupby(['Location', 'Standardized_Job_Title']).size().rename('Terminations_FTE')\n",
    "    terminations_prn_by_facility_role = terminations_filtered_prn.groupby(['Location', 'Standardized_Job_Title']).size().rename('Terminations_PRN')\n",
    "    \n",
    "    # Requisitions by facility and job title\n",
    "    if 'Req_Status' in requisitions_filtered.columns:\n",
    "        open_statuses = ANALYSIS_CONFIG['open_req_statuses']\n",
    "        closed_statuses = ANALYSIS_CONFIG['closed_req_statuses']\n",
    "        \n",
    "        requisitions_by_facility_role = requisitions_filtered.groupby(['Location', 'Standardized_Job_Title']).agg(\n",
    "            Open_Requisitions=('Req_Status', lambda x: x.isin(open_statuses).sum()),\n",
    "            Closed_Requisitions=('Req_Status', lambda x: x.isin(closed_statuses).sum()),\n",
    "            Total_Requisitions=('Req_Status', 'count')\n",
    "        )\n",
    "    else:\n",
    "        requisitions_by_facility_role = requisitions_filtered.groupby(['Location', 'Standardized_Job_Title']).size().to_frame('Total_Requisitions')\n",
    "        requisitions_by_facility_role['Open_Requisitions'] = 0\n",
    "        requisitions_by_facility_role['Closed_Requisitions'] = 0\n",
    "    \n",
    "    # Combine all facility-level metrics\n",
    "    facility_job_metrics = pd.DataFrame({\n",
    "        'Headcount_All': headcount_by_facility_role,\n",
    "        'Headcount_FTE': headcount_fte_by_facility_role,\n",
    "        'Headcount_PRN': headcount_prn_by_facility_role,\n",
    "        'Hires_All': hires_by_facility_role,\n",
    "        'Hires_FTE': hires_fte_by_facility_role,\n",
    "        'Hires_PRN': hires_prn_by_facility_role,\n",
    "        'Terminations_All': terminations_by_facility_role,\n",
    "        'Terminations_FTE': terminations_fte_by_facility_role,\n",
    "        'Terminations_PRN': terminations_prn_by_facility_role,\n",
    "        'Open_Requisitions': requisitions_by_facility_role['Open_Requisitions'],\n",
    "        'Closed_Requisitions': requisitions_by_facility_role['Closed_Requisitions'],\n",
    "        'Total_Requisitions': requisitions_by_facility_role['Total_Requisitions']\n",
    "    }).fillna(0)\n",
    "    \n",
    "    # Calculate derived metrics\n",
    "    facility_job_metrics['Net_Change_FTE'] = facility_job_metrics['Hires_FTE'] - facility_job_metrics['Terminations_FTE']\n",
    "    \n",
    "    facility_job_metrics['Turnover_Rate_FTE_Percent'] = facility_job_metrics.apply(\n",
    "        lambda row: (row['Terminations_FTE'] / row['Headcount_FTE'] * 100) if row['Headcount_FTE'] > 0 else 0,\n",
    "        axis=1\n",
    "    ).round(1)\n",
    "    \n",
    "    facility_job_metrics['Vacancy_Rate_Percent'] = facility_job_metrics.apply(\n",
    "        lambda row: (row['Open_Requisitions'] / row['Headcount_All'] * 100) if row['Headcount_All'] > 0 else 0,\n",
    "        axis=1\n",
    "    ).round(1)\n",
    "    \n",
    "    # Reset index to make facility and job title regular columns\n",
    "    facility_job_metrics = facility_job_metrics.reset_index()\n",
    "    \n",
    "    # Add facility type information\n",
    "    facility_type_lookup = headcount_filtered[['Location', 'Facility_Type', 'Facility_Sub_Type']].drop_duplicates()\n",
    "    facility_job_metrics = facility_job_metrics.merge(\n",
    "        facility_type_lookup,\n",
    "        on='Location',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Reorder columns\n",
    "    column_order = [\n",
    "        'Location', 'Facility_Type', 'Facility_Sub_Type', 'Standardized_Job_Title',\n",
    "        'Headcount_All', 'Headcount_FTE', 'Headcount_PRN',\n",
    "        'Hires_All', 'Hires_FTE', 'Hires_PRN',\n",
    "        'Terminations_All', 'Terminations_FTE', 'Terminations_PRN',\n",
    "        'Net_Change_FTE', 'Open_Requisitions', 'Closed_Requisitions', 'Total_Requisitions',\n",
    "        'Turnover_Rate_FTE_Percent', 'Vacancy_Rate_Percent'\n",
    "    ]\n",
    "    \n",
    "    facility_job_metrics = facility_job_metrics[column_order]\n",
    "    \n",
    "    # Sort by facility type, facility name, then job title\n",
    "    facility_job_metrics = facility_job_metrics.sort_values(['Facility_Type', 'Location', 'Standardized_Job_Title'])\n",
    "    \n",
    "    print(f\"Facility-level analysis complete:\")\n",
    "    print(f\"  Total facility-job title combinations: {len(facility_job_metrics)}\")\n",
    "    print(f\"  Unique facilities: {facility_job_metrics['Location'].nunique()}\")\n",
    "    print(f\"  Job titles: {facility_job_metrics['Standardized_Job_Title'].nunique()}\")\n",
    "    print(f\"  Facility types: {facility_job_metrics['Facility_Type'].nunique()}\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(f\"\\nSample facility metrics (first 10 rows):\")\n",
    "    sample_cols = ['Location', 'Facility_Type', 'Standardized_Job_Title', 'Headcount_All', 'Hires_All', 'Terminations_All']\n",
    "    display(facility_job_metrics[sample_cols].head(10))\n",
    "    \n",
    "    # Validation - ensure totals still match\n",
    "    facility_headcount_total = facility_job_metrics['Headcount_All'].sum()\n",
    "    facility_hires_total = facility_job_metrics['Hires_All'].sum()\n",
    "    facility_terms_total = facility_job_metrics['Terminations_All'].sum()\n",
    "    \n",
    "    print(f\"\\nVALIDATION - Facility totals vs system totals:\")\n",
    "    print(f\"  Headcount: {facility_headcount_total} (should equal {len(headcount_filtered)})\")\n",
    "    print(f\"  Hires: {facility_hires_total} (should equal {len(hires_filtered)})\")\n",
    "    print(f\"  Terminations: {facility_terms_total} (should equal {len(terminations_filtered)})\")\n",
    "    \n",
    "    if (facility_headcount_total != len(headcount_filtered) or\n",
    "        facility_hires_total != len(hires_filtered) or\n",
    "        facility_terms_total != len(terminations_filtered)):\n",
    "        print(\"✗ CRITICAL: Facility totals don't match system totals!\")\n",
    "        raise ValueError(\"Facility-level aggregation failed validation\")\n",
    "    \n",
    "    print(\"✓ Facility-level metrics validation passed\")\n",
    "    return facility_job_metrics\n",
    "\n",
    "facility_job_metrics = create_facility_level_metrics()\n",
    "\n",
    "print(\"\\n✓ Facility-level analysis setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPARATION COMPLETE\n",
    "All datasets have been loaded, validated, filtered, and standardized.\n",
    "All metrics have been calculated and validated.\n",
    "Ready for output generation.\n",
    "\n",
    "## FINAL DATASET SUMMARY:\n",
    "The following dataframes are now available for use:\n",
    "- `headcount_filtered`: {len(headcount_filtered)} rows\n",
    "- `hires_filtered`: {len(hires_filtered)} rows\n",
    "- `terminations_filtered`: {len(terminations_filtered)} rows\n",
    "- `requisitions_filtered`: {len(requisitions_filtered)} rows\n",
    "- `job_title_summary`: {len(job_title_summary)} rows\n",
    "- `facility_job_metrics`: {len(facility_job_metrics)} rows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
