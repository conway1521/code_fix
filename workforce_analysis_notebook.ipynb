  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 9: Facility-Level Analysis Setup\n",
    "\n",
    "Create comprehensive facility-level workforce metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 9: FACILITY-LEVEL ANALYSIS SETUP\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def create_facility_level_metrics():\n",
    "    \"\"\"Create comprehensive facility-level workforce metrics\"\"\"\n",
    "    print(\"Creating facility-level metrics by job title...\")\n",
    "    \n",
    "    # Calculate metrics by facility and job title\n",
    "    headcount_by_facility_role = headcount_filtered.groupby(['Location', 'Standardized_Job_Title']).size().rename('Headcount_All')\n",
    "    headcount_fte_by_facility_role = headcount_filtered_fte.groupby(['Location', 'Standardized_Job_Title']).size().rename('Headcount_FTE')\n",
    "    headcount_prn_by_facility_role = headcount_filtered_prn.groupby(['Location', 'Standardized_Job_Title']).size().rename('Headcount_PRN')\n",
    "    \n",
    "    hires_by_facility_role = hires_filtered.groupby(['Location', 'Standardized_Job_Title']).size().rename('Hires_All')\n",
    "    hires_fte_by_facility_role = hires_filtered_fte.groupby(['Location', 'Standardized_Job_Title']).size().rename('Hires_FTE')\n",
    "    hires_prn_by_facility_role = hires_filtered_prn.groupby(['Location', 'Standardized_Job_Title']).size().rename('Hires_PRN')\n",
    "    \n",
    "    terminations_by_facility_role = terminations_filtered.groupby(['Location', 'Standardized_Job_Title']).size().rename('Terminations_All')\n",
    "    terminations_fte_by_facility_role = terminations_filtered_fte.groupby(['Location', 'Standardized_Job_Title']).size().rename('Terminations_FTE')\n",
    "    terminations_prn_by_facility_role = terminations_filtered_prn.groupby(['Location', 'Standardized_Job_Title']).size().rename('Terminations_PRN')\n",
    "    \n",
    "    # Requisitions by facility and job title\n",
    "    if 'Req_Status' in requisitions_filtered.columns:\n",
    "        open_statuses = ANALYSIS_CONFIG['open_req_statuses']\n",
    "        closed_statuses = ANALYSIS_CONFIG['closed_req_statuses']\n",
    "        \n",
    "        requisitions_by_facility_role = requisitions_filtered.groupby(['Location', 'Standardized_Job_Title']).agg(\n",
    "            Open_Requisitions=('Req_Status', lambda x: x.isin(open_statuses).sum()),\n",
    "            Closed_Requisitions=('Req_Status', lambda x: x.isin(closed_statuses).sum()),\n",
    "            Total_Requisitions=('Req_Status', 'count')\n",
    "        )\n",
    "    else:\n",
    "        requisitions_by_facility_role = requisitions_filtered.groupby(['Location', 'Standardized_Job_Title']).size().to_frame('Total_Requisitions')\n",
    "        requisitions_by_facility_role['Open_Requisitions'] = 0\n",
    "        requisitions_by_facility_role['Closed_Requisitions'] = 0\n",
    "    \n",
    "    # Combine all facility-level metrics\n",
    "    facility_job_metrics = pd.DataFrame({\n",
    "        'Headcount_All': headcount_by_facility_role,\n",
    "        'Headcount_FTE': headcount_fte_by_facility_role,\n",
    "        'Headcount_PRN': headcount_prn_by_facility_role,\n",
    "        'Hires_All': hires_by_facility_role,\n",
    "        'Hires_FTE': hires_fte_by_facility_role,\n",
    "        'Hires_PRN': hires_prn_by_facility_role,\n",
    "        'Terminations_All': terminations_by_facility_role,\n",
    "        'Terminations_FTE': terminations_fte_by_facility_role,\n",
    "        'Terminations_PRN': terminations_prn_by_facility_role,\n",
    "        'Open_Requisitions': requisitions_by_facility_role['Open_Requisitions'],\n",
    "        'Closed_Requisitions': requisitions_by_facility_role['Closed_Requisitions'],\n",
    "        'Total_Requisitions': requisitions_by_facility_role['Total_Requisitions']\n",
    "    }).fillna(0)\n",
    "    \n",
    "    # Calculate derived metrics\n",
    "    facility_job_metrics['Net_Change_FTE'] = facility_job_metrics['Hires_FTE'] - facility_job_metrics['Terminations_FTE']\n",
    "    \n",
    "    facility_job_metrics['Turnover_Rate_FTE_Percent'] = facility_job_metrics.apply(\n",
    "        lambda row: (row['Terminations_FTE'] / row['Headcount_FTE'] * 100) if row['Headcount_FTE'] > 0 else 0,\n",
    "        axis=1\n",
    "    ).round(1)\n",
    "    \n",
    "    facility_job_metrics['Vacancy_Rate_Percent'] = facility_job_metrics.apply(\n",
    "        lambda row: (row['Open_Requisitions'] / row['Headcount_All'] * 100) if row['Headcount_All'] > 0 else 0,\n",
    "        axis=1\n",
    "    ).round(1)\n",
    "    \n",
    "    # Reset index to make facility and job title regular columns\n",
    "    facility_job_metrics = facility_job_metrics.reset_index()\n",
    "    \n",
    "    # Add facility type information\n",
    "    facility_type_lookup = headcount_filtered[['Location', 'Facility_Type', 'Facility_Sub_Type']].drop_duplicates()\n",
    "    facility_job_metrics = facility_job_metrics.merge(\n",
    "        facility_type_lookup,\n",
    "        on='Location',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Reorder columns\n",
    "    column_order = [\n",
    "        'Location', 'Facility_Type', 'Facility_Sub_Type', 'Standardized_Job_Title',\n",
    "        'Headcount_All', 'Headcount_FTE', 'Headcount_PRN',\n",
    "        'Hires_All', 'Hires_FTE', 'Hires_PRN',\n",
    "        'Terminations_All', 'Terminations_FTE', 'Terminations_PRN',\n",
    "        'Net_Change_FTE', 'Open_Requisitions', 'Closed_Requisitions', 'Total_Requisitions',\n",
    "        'Turnover_Rate_FTE_Percent', 'Vacancy_Rate_Percent'\n",
    "    ]\n",
    "    \n",
    "    facility_job_metrics = facility_job_metrics[column_order]\n",
    "    \n",
    "    # Sort by facility type, facility name, then job title\n",
    "    facility_job_metrics = facility_job_metrics.sort_values(['Facility_Type', 'Location', 'Standardized_Job_Title'])\n",
    "    \n",
    "    print(f\"   ✓ Facility-level combinations: {len(facility_job_metrics)} facility-job combinations\")\n",
    "    \n",
    "    # 4. Employment status breakdown\n",
    "    print(f\"\\n4. Employment Status Breakdown:\")\n",
    "    total_fte = system_overview['Total_Headcount_FTE']\n",
    "    total_prn = system_overview['Total_Headcount_PRN']\n",
    "    total_all = system_overview['Total_Headcount_All']\n",
    "    \n",
    "    print(f\"   ✓ FTE employees: {total_fte:,} ({total_fte/total_all*100:.1f}%)\")\n",
    "    print(f\"   ✓ PRN employees: {total_prn:,} ({total_prn/total_all*100:.1f}%)\")\n",
    "    print(f\"   ✓ Total employees: {total_all:,}\")\n",
    "    print(f\"   ✓ Contingent ratio: {system_overview['Contingent_to_FTE_Ratio']}\")\n",
    "    \n",
    "    # 5. Key insights\n",
    "    print(f\"\\n5. Key Insights:\")\n",
    "    churn_rate = system_overview['Churn_Rate_FTE_Percent']\n",
    "    vacancy_rate = system_overview['Vacancy_Rate_Percent']\n",
    "    net_change = system_overview['Net_Change_FTE']\n",
    "    \n",
    "    print(f\"   ✓ FTE churn rate: {churn_rate}%\")\n",
    "    print(f\"   ✓ Vacancy rate: {vacancy_rate}%\")\n",
    "    print(f\"   ✓ Net FTE change: {net_change:+,}\")\n",
    "    print(f\"   ✓ Open requisitions: {system_overview['Open_Requisitions']:,}\")\n",
    "    \n",
    "    # 6. Quality assessment\n",
    "    print(f\"\\n6. Quality Assessment:\")\n",
    "    \n",
    "    # Check for potential data quality issues\n",
    "    quality_issues = []\n",
    "    \n",
    "    # Very high turnover rates\n",
    "    high_turnover_roles = job_title_summary[job_title_summary['Turnover_Rate_FTE_Percent'] > 50]\n",
    "    if len(high_turnover_roles) > 0:\n",
    "        quality_issues.append(f\"High turnover rates (>50%) found for: {list(high_turnover_roles.index)}\")\n",
    "    \n",
    "    # Very high vacancy rates\n",
    "    high_vacancy_roles = job_title_summary[job_title_summary['Vacancy_Rate_Percent'] > 30]\n",
    "    if len(high_vacancy_roles) > 0:\n",
    "        quality_issues.append(f\"High vacancy rates (>30%) found for: {list(high_vacancy_roles.index)}\")\n",
    "    \n",
    "    # Facilities with no requisitions\n",
    "    facilities_no_reqs = facility_job_metrics[facility_job_metrics['Total_Requisitions'] == 0]['Location'].nunique()\n",
    "    total_facilities = facility_job_metrics['Location'].nunique()\n",
    "    if facilities_no_reqs > total_facilities * 0.5:\n",
    "        quality_issues.append(f\"Many facilities ({facilities_no_reqs}/{total_facilities}) have no requisitions data\")\n",
    "    \n",
    "    if quality_issues:\n",
    "        print(\"   ⚠ Potential quality issues identified:\")\n",
    "        for issue in quality_issues:\n",
    "            print(f\"     - {issue}\")\n",
    "    else:\n",
    "        print(\"   ✓ No major quality issues identified\")\n",
    "    \n",
    "    print(f\"\\n✓ ANALYSIS COMPLETE\")\n",
    "    print(f\"✓ All validations passed\")\n",
    "    print(f\"✓ {len(output_files)} output files generated\")\n",
    "    \n",
    "    return quality_issues\n",
    "\n",
    "# Run final validation\n",
    "quality_issues = final_validation_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"HEALTHCARE WORKFORCE ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\"\"\n",
    "ANALYSIS SUMMARY:\n",
    "================\n",
    "Target State: {ANALYSIS_CONFIG['target_state']}\n",
    "Analysis Date: {ANALYSIS_CONFIG['analysis_date']}\n",
    "Job Titles Analyzed: {len(job_title_summary)}\n",
    "Facilities Analyzed: {facility_job_metrics['Location'].nunique()}\n",
    "Total Employees: {system_overview['Total_Headcount_All']:,}\n",
    "\n",
    "KEY METRICS:\n",
    "============\n",
    "FTE Employees: {system_overview['Total_Headcount_FTE']:,}\n",
    "PRN Employees: {system_overview['Total_Headcount_PRN']:,}\n",
    "Annual Hires: {system_overview['Total_Hires_All']:,}\n",
    "Annual Terminations: {system_overview['Total_Terminations_All']:,}\n",
    "Open Requisitions: {system_overview['Open_Requisitions']:,}\n",
    "Churn Rate (FTE): {system_overview['Churn_Rate_FTE_Percent']}%\n",
    "Vacancy Rate: {system_overview['Vacancy_Rate_Percent']}%\n",
    "\n",
    "OUTPUT FILES GENERATED:\n",
    "======================\n",
    "\"\"\")\n",
    "\n",
    "for file_type, filename in output_files.items():\n",
    "    print(f\"- {file_type}: {filename}\")\n",
    "\n",
    "if quality_issues:\n",
    "    print(f\"\\nDATA QUALITY NOTES:\")\n",
    "    print(\"====================\")\n",
    "    for issue in quality_issues:\n",
    "        print(f\"⚠ {issue}\")\n",
    "\n",
    "print(f\"\\n🎯 Analysis ready for review and further investigation!\")\n",
    "print(f\"📊 Use the generated files for reporting, dashboards, and deeper analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template Usage Instructions for Future Analyses\n",
    "\n",
    "## TO USE THIS TEMPLATE FOR A NEW HEALTH SYSTEM:\n",
    "\n",
    "### 1. UPDATE CONFIGURATION (Section 1):\n",
    "- Modify `DATA_CONFIG` paths and sheet names\n",
    "- Update `COLUMN_MAPPING` to match your data columns\n",
    "- Adjust `ANALYSIS_CONFIG` parameters as needed\n",
    "\n",
    "### 2. REQUIRED DATA STRUCTURE:\n",
    "- **Headcount**: Job titles, locations, employment status, state\n",
    "- **Hires**: Job titles, locations, hire dates, state  \n",
    "- **Terminations**: Job titles, locations, termination dates, state\n",
    "- **Requisitions**: Job titles, locations, requisition status\n",
    "\n",
    "### 3. VALIDATION APPROACH:\n",
    "- Run each section sequentially\n",
    "- Check validation output after each section\n",
    "- Address any critical errors before proceeding\n",
    "- Review data quality report before finalizing\n",
    "\n",
    "### 4. CUSTOMIZATION POINTS:\n",
    "- Job title standardization function (Section 5)\n",
    "- Facility classification function (Section 7)\n",
    "- Requisition status mappings (Section 8)\n",
    "- Output file naming and structure (Section 10)\n",
    "\n",
    "### 5. QUALITY CHECKS:\n",
    "- Capture rates should be >10% for job title filtering\n",
    "- Employment status distribution should make sense\n",
    "- Totals should always match across aggregation levels\n",
    "- Review facility classification results\n",
    "\n",
    "### 6. OUTPUT USAGE:\n",
    "- **system_overview**: Executive dashboard metrics\n",
    "- **job_title_summary**: Role-specific workforce analysis\n",
    "- **facility_analysis**: Operational planning and management\n",
    "- **quality_report**: Data validation and methodology notes\n",
    "\n",
    "**Remember**: This template prioritizes accuracy over automation. When data quality is insufficient, it will fail loudly rather than produce questionable results."
   ]
  }\n",
 ],\n",
 \"metadata\": {\n",
  \"kernelspec\": {\n",
   \"display_name\": \"Python 3\",\n",
   \"language\": \"python\",\n",
   \"name\": \"python3\"\n",
  },\n",
  \"language_info\": {\n",
   \"codemirror_mode\": {\n",
    \"name\": \"ipython\",\n",
    \"version\": 3\n",
   },\n",
   \"file_extension\": \".py\",\n",
   \"mimetype\": \"text/x-python\",\n",
   \"name\": \"python\",\n",
   \"nbconvert_exporter\": \"python\",\n",
   \"pygments_lexer\": \"ipython3\",\n",
   \"version\": \"3.8.5\"\n",
  }\n",
 },\n",
 \"nbformat\": 4,\n",
 \"nbformat_minor\": 4\n}\"Facility-level analysis complete:\")\n",
    "    print(f\"  Total facility-job title combinations: {len(facility_job_metrics)}\")\n",
    "    print(f\"  Unique facilities: {facility_job_metrics['Location'].nunique()}\")\n",
    "    print(f\"  Job titles: {facility_job_metrics['Standardized_Job_Title'].nunique()}\")\n",
    "    print(f\"  Facility types: {facility_job_metrics['Facility_Type'].nunique()}\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(f\"\\nSample facility metrics (first 10 rows):\")\n",
    "    sample_cols = ['Location', 'Facility_Type', 'Standardized_Job_Title', 'Headcount_All', 'Hires_All', 'Terminations_All']\n",
    "    display(facility_job_metrics[sample_cols].head(10))\n",
    "    \n",
    "    # Validation - ensure totals still match\n",
    "    facility_headcount_total = facility_job_metrics['Headcount_All'].sum()\n",
    "    facility_hires_total = facility_job_metrics['Hires_All'].sum()\n",
    "    facility_terms_total = facility_job_metrics['Terminations_All'].sum()\n",
    "    \n",
    "    print(f\"\\nVALIDATION - Facility totals vs system totals:\")\n",
    "    print(f\"  Headcount: {facility_headcount_total} (should equal {len(headcount_filtered)})\")\n",
    "    print(f\"  Hires: {facility_hires_total} (should equal {len(hires_filtered)})\")\n",
    "    print(f\"  Terminations: {facility_terms_total} (should equal {len(terminations_filtered)})\")\n",
    "    \n",
    "    if (facility_headcount_total != len(headcount_filtered) or\n",
    "        facility_hires_total != len(hires_filtered) or\n",
    "        facility_terms_total != len(terminations_filtered)):\n",
    "        print(\"✗ CRITICAL: Facility totals don't match system totals!\")\n",
    "        raise ValueError(\"Facility-level aggregation failed validation\")\n",
    "    \n",
    "    print(\"✓ Facility-level metrics validation passed\")\n",
    "    return facility_job_metrics\n",
    "\n",
    "# Create facility-level metrics\n",
    "facility_job_metrics = create_facility_level_metrics()\n",
    "\n",
    "print(\"\\n✓ Facility-level analysis setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 10: Output Generation Framework\n",
    "\n",
    "Generate standardized output files for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 10: OUTPUT GENERATION FRAMEWORK\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def generate_outputs():\n",
    "    \"\"\"Generate standardized output files for analysis\"\"\"\n",
    "    \n",
    "    # Configure output folder\n",
    "    output_folder = DATA_CONFIG.get('output_folder', DATA_CONFIG['base_folder'])\n",
    "    \n",
    "    # Create timestamp for unique file names\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    print(f\"Generating outputs in folder: {output_folder}\")\n",
    "    \n",
    "    # 1. System Overview CSV\n",
    "    print(\"\\n1. Creating system overview file...\")\n",
    "    system_overview_df = pd.DataFrame([system_overview])\n",
    "    system_overview_filename = f\"{output_folder}/system_overview_{timestamp}.csv\"\n",
    "    system_overview_df.to_csv(system_overview_filename, index=False, encoding='utf-8-sig')\n",
    "    print(f\"✓ System overview saved: {system_overview_filename}\")\n",
    "    \n",
    "    # 2. Job Title Summary CSV\n",
    "    print(\"\\n2. Creating job title summary file...\")\n",
    "    job_title_filename = f\"{output_folder}/job_title_summary_{timestamp}.csv\"\n",
    "    job_title_summary.to_csv(job_title_filename, index=True, encoding='utf-8-sig')\n",
    "    print(f\"✓ Job title summary saved: {job_title_filename}\")\n",
    "    \n",
    "    # 3. Facility-Level Analysis CSV\n",
    "    print(\"\\n3. Creating facility-level analysis file...\")\n",
    "    facility_filename = f\"{output_folder}/facility_workforce_analysis_{timestamp}.csv\"\n",
    "    facility_job_metrics.to_csv(facility_filename, index=False, encoding='utf-8-sig')\n",
    "    print(f\"✓ Facility analysis saved: {facility_filename}\")\n",
    "    \n",
    "    # 4. Data Quality Report\n",
    "    print(\"\\n4. Creating data quality report...\")\n",
    "    \n",
    "    data_quality_report = {\n",
    "        'Analysis_Timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        'Configuration_Target_State': ANALYSIS_CONFIG['target_state'],\n",
    "        'Configuration_Analysis_Date': ANALYSIS_CONFIG['analysis_date'],\n",
    "        'Configuration_Target_Job_Titles': str(ANALYSIS_CONFIG['target_job_titles']),\n",
    "        'Data_Sources_Headcount_File': DATA_CONFIG['headcount_source']['file'],\n",
    "        'Data_Sources_Hires_File': DATA_CONFIG['hires_source']['file'],\n",
    "        'Data_Sources_Terminations_File': DATA_CONFIG['terminations_source']['file'],\n",
    "        'Data_Sources_Requisitions_File': DATA_CONFIG['requisitions_source']['file'],\n",
    "        'Filtered_Headcount_Records': len(headcount_filtered),\n",
    "        'Filtered_Hires_Records': len(hires_filtered),\n",
    "        'Filtered_Terminations_Records': len(terminations_filtered),\n",
    "        'Filtered_Requisitions_Records': len(requisitions_filtered),\n",
    "        'Unique_Facilities': facility_job_metrics['Location'].nunique(),\n",
    "        'Unique_Job_Titles': len(ANALYSIS_CONFIG['target_job_titles']),\n",
    "        'Facility_Types_Found': facility_job_metrics['Facility_Type'].nunique(),\n",
    "        'All_Validations_Passed': True\n",
    "    }\n",
    "    \n",
    "    # Convert to DataFrame for easier viewing\n",
    "    quality_report_df = pd.DataFrame([data_quality_report])\n",
    "    quality_filename = f\"{output_folder}/data_quality_report_{timestamp}.csv\"\n",
    "    quality_report_df.to_csv(quality_filename, index=False, encoding='utf-8-sig')\n",
    "    print(f\"✓ Data quality report saved: {quality_filename}\")\n",
    "    \n",
    "    # 5. Analysis-Ready Datasets (optional - for further analysis)\n",
    "    print(\"\\n5. Creating analysis-ready datasets...\")\n",
    "    \n",
    "    # Save filtered datasets for potential further analysis\n",
    "    datasets_filename = f\"{output_folder}/analysis_datasets_{timestamp}.xlsx\"\n",
    "    \n",
    "    with pd.ExcelWriter(datasets_filename, engine='openpyxl') as writer:\n",
    "        headcount_filtered.to_excel(writer, sheet_name='Headcount_Filtered', index=False)\n",
    "        hires_filtered.to_excel(writer, sheet_name='Hires_Filtered', index=False)\n",
    "        terminations_filtered.to_excel(writer, sheet_name='Terminations_Filtered', index=False)\n",
    "        requisitions_filtered.to_excel(writer, sheet_name='Requisitions_Filtered', index=False)\n",
    "        \n",
    "        # Also save the FTE/PRN subsets\n",
    "        headcount_filtered_fte.to_excel(writer, sheet_name='Headcount_FTE', index=False)\n",
    "        headcount_filtered_prn.to_excel(writer, sheet_name='Headcount_PRN', index=False)\n",
    "        hires_filtered_fte.to_excel(writer, sheet_name='Hires_FTE', index=False)\n",
    "        hires_filtered_prn.to_excel(writer, sheet_name='Hires_PRN', index=False)\n",
    "        terminations_filtered_fte.to_excel(writer, sheet_name='Terminations_FTE', index=False)\n",
    "        terminations_filtered_prn.to_excel(writer, sheet_name='Terminations_PRN', index=False)\n",
    "        \n",
    "        if has_contractors:\n",
    "            contractors_filtered.to_excel(writer, sheet_name='Contractors_Filtered', index=False)\n",
    "    \n",
    "    print(f\"✓ Analysis-ready datasets saved: {datasets_filename}\")\n",
    "    \n",
    "    # 6. Summary Dashboard Data (JSON format for easy consumption)\n",
    "    print(\"\\n6. Creating dashboard summary...\")\n",
    "    \n",
    "    dashboard_data = {\n",
    "        'metadata': {\n",
    "            'analysis_date': ANALYSIS_CONFIG['analysis_date'],\n",
    "            'target_state': ANALYSIS_CONFIG['target_state'],\n",
    "            'generated_timestamp': datetime.now().isoformat(),\n",
    "            'total_facilities': int(facility_job_metrics['Location'].nunique()),\n",
    "            'job_titles_analyzed': sorted(headcount_filtered['Standardized_Job_Title'].unique())\n",
    "        },\n",
    "        'system_metrics': system_overview,\n",
    "        'job_title_breakdown': job_title_summary.round(1).to_dict('index'),\n",
    "        'facility_type_summary': facility_job_metrics.groupby('Facility_Type').agg({\n",
    "            'Headcount_All': 'sum',\n",
    "            'Hires_All': 'sum', \n",
    "            'Terminations_All': 'sum',\n",
    "            'Open_Requisitions': 'sum'\n",
    "        }).round(1).to_dict('index'),\n",
    "        'top_facilities_by_headcount': facility_job_metrics.groupby('Location')['Headcount_All'].sum().nlargest(10).to_dict()\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    dashboard_filename = f\"{output_folder}/dashboard_data_{timestamp}.json\"\n",
    "    with open(dashboard_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(dashboard_data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"✓ Dashboard data saved: {dashboard_filename}\")\n",
    "    \n",
    "    print(f\"\\n✓ Output generation complete!\")\n",
    "    print(f\"Generated files:\")\n",
    "    print(f\"  - {system_overview_filename}\")\n",
    "    print(f\"  - {job_title_filename}\")\n",
    "    print(f\"  - {facility_filename}\")\n",
    "    print(f\"  - {quality_filename}\")\n",
    "    print(f\"  - {datasets_filename}\")\n",
    "    print(f\"  - {dashboard_filename}\")\n",
    "    \n",
    "    return {\n",
    "        'system_overview': system_overview_filename,\n",
    "        'job_title_summary': job_title_filename,\n",
    "        'facility_analysis': facility_filename,\n",
    "        'quality_report': quality_filename,\n",
    "        'analysis_datasets': datasets_filename,\n",
    "        'dashboard_data': dashboard_filename\n",
    "    }\n",
    "\n",
    "# Generate all outputs\n",
    "output_files = generate_outputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 11: Final Validation & Summary\n",
    "\n",
    "Perform final validation checks and provide analysis summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 11: FINAL VALIDATION & SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def final_validation_summary():\n",
    "    \"\"\"Perform final validation checks and provide analysis summary\"\"\"\n",
    "    \n",
    "    print(\"FINAL VALIDATION CHECKS:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # 1. Data completeness checks\n",
    "    print(\"1. Data Completeness:\")\n",
    "    print(f\"   ✓ Headcount records processed: {len(headcount_filtered):,}\")\n",
    "    print(f\"   ✓ Hires records processed: {len(hires_filtered):,}\")\n",
    "    print(f\"   ✓ Terminations records processed: {len(terminations_filtered):,}\")\n",
    "    print(f\"   ✓ Requisitions records processed: {len(requisitions_filtered):,}\")\n",
    "    \n",
    "    # 2. Standardization success\n",
    "    print(f\"\\n2. Standardization Success:\")\n",
    "    job_titles_found = sorted(headcount_filtered['Standardized_Job_Title'].unique())\n",
    "    target_titles = ANALYSIS_CONFIG['target_job_titles']\n",
    "    print(f\"   ✓ Target job titles: {len(target_titles)}\")\n",
    "    print(f\"   ✓ Job titles found: {len(job_titles_found)}\")\n",
    "    print(f\"   ✓ Missing titles: {set(target_titles) - set(job_titles_found)}\")\n",
    "    \n",
    "    facilities_classified = facility_job_metrics['Facility_Type'].value_counts()\n",
    "    print(f\"   ✓ Facilities classified: {facility_job_metrics['Location'].nunique()}\")\n",
    "    print(f\"   ✓ Facility types: {dict(facilities_classified)}\")\n",
    "    \n",
    "    # 3. Metric calculations\n",
    "    print(f\"\\n3. Metric Calculations:\")\n",
    "    print(f\"   ✓ System-wide metrics: {len(system_overview)} metrics calculated\")\n",
    "    print(f\"   ✓ Job title metrics: {len(job_title_summary)} job titles analyzed\")\n",
    "    print(f  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6: Employment Status Standardization\n",
    "\n",
    "Create standardized FTE/PRN employment status categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 6: EMPLOYMENT STATUS STANDARDIZATION\")  \n",
    "print(\"=\" * 80)\n",
    "\n",
    "def create_standardized_employment_status(df, dataset_name):\n",
    "    \"\"\"Create standardized FTE/PRN employment status\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Check required columns exist\n",
    "    if 'Full_Part_Time' not in df.columns or 'Employment_Category' not in df.columns:\n",
    "        print(f\"✗ CRITICAL: Missing employment status columns in {dataset_name}\")\n",
    "        raise ValueError(f\"Employment status columns missing in {dataset_name}\")\n",
    "    \n",
    "    # Show original distributions\n",
    "    print(f\"\\n{dataset_name} employment status standardization:\")\n",
    "    ft_dist = df['Full_Part_Time'].value_counts()\n",
    "    emp_dist = df['Employment_Category'].value_counts()\n",
    "    print(f\"  Full_Part_Time distribution: {dict(ft_dist)}\")\n",
    "    print(f\"  Employment_Category distribution: {dict(emp_dist)}\")\n",
    "    \n",
    "    # Create standardized status\n",
    "    df['Standardized_Employment_Status'] = ''\n",
    "    \n",
    "    # Set FTE for full-time employees\n",
    "    fte_values = ANALYSIS_CONFIG['fte_values']\n",
    "    fte_mask = df['Full_Part_Time'].isin(fte_values)\n",
    "    df.loc[fte_mask, 'Standardized_Employment_Status'] = 'FTE'\n",
    "    \n",
    "    # Override with PRN where applicable  \n",
    "    prn_values = ANALYSIS_CONFIG['prn_values']\n",
    "    prn_mask = df['Employment_Category'].isin(prn_values)\n",
    "    df.loc[prn_mask, 'Standardized_Employment_Status'] = 'PRN'\n",
    "    \n",
    "    # Validation\n",
    "    fte_count = (df['Standardized_Employment_Status'] == 'FTE').sum()\n",
    "    prn_count = (df['Standardized_Employment_Status'] == 'PRN').sum()\n",
    "    other_count = len(df) - fte_count - prn_count\n",
    "    \n",
    "    print(f\"  Standardized employment status:\")\n",
    "    print(f\"    FTE: {fte_count}\")\n",
    "    print(f\"    PRN: {prn_count}\")\n",
    "    print(f\"    Other/Missing: {other_count}\")\n",
    "    \n",
    "    if other_count > 0:\n",
    "        print(f\"✗ WARNING: {other_count} records with unclear employment status in {dataset_name}\")\n",
    "        # Show examples\n",
    "        other_examples = df[~df['Standardized_Employment_Status'].isin(['FTE', 'PRN'])][\n",
    "            ['Full_Part_Time', 'Employment_Category']].drop_duplicates().head(5)\n",
    "        print(f\"Examples of unclear status:\")\n",
    "        display(other_examples)\n",
    "    \n",
    "    if fte_count == 0 and prn_count == 0:\n",
    "        print(f\"✗ CRITICAL: No valid employment status found in {dataset_name}\")\n",
    "        raise ValueError(f\"Employment status standardization failed for {dataset_name}\")\n",
    "    \n",
    "    print(f\"✓ {dataset_name} employment status standardization complete\")\n",
    "    return df\n",
    "\n",
    "# Apply employment status standardization\n",
    "print(\"Creating standardized employment status...\")\n",
    "print(f\"FTE values: {ANALYSIS_CONFIG['fte_values']}\")\n",
    "print(f\"PRN values: {ANALYSIS_CONFIG['prn_values']}\")\n",
    "\n",
    "headcount_filtered = create_standardized_employment_status(headcount_filtered, 'headcount')\n",
    "hires_filtered = create_standardized_employment_status(hires_filtered, 'hires')\n",
    "terminations_filtered = create_standardized_employment_status(terminations_filtered, 'terminations')\n",
    "\n",
    "if has_contractors:\n",
    "    contractors_filtered = create_standardized_employment_status(contractors_filtered, 'contractors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FTE and PRN subsets for analysis\n",
    "print(\"\\nCreating employment status subsets...\")\n",
    "\n",
    "# Headcount subsets\n",
    "headcount_filtered_fte = headcount_filtered[headcount_filtered['Standardized_Employment_Status'] == 'FTE'].copy()\n",
    "headcount_filtered_prn = headcount_filtered[headcount_filtered['Standardized_Employment_Status'] == 'PRN'].copy()\n",
    "\n",
    "# Hires subsets  \n",
    "hires_filtered_fte = hires_filtered[hires_filtered['Standardized_Employment_Status'] == 'FTE'].copy()\n",
    "hires_filtered_prn = hires_filtered[hires_filtered['Standardized_Employment_Status'] == 'PRN'].copy()\n",
    "\n",
    "# Terminations subsets\n",
    "terminations_filtered_fte = terminations_filtered[terminations_filtered['Standardized_Employment_Status'] == 'FTE'].copy()\n",
    "terminations_filtered_prn = terminations_filtered[terminations_filtered['Standardized_Employment_Status'] == 'PRN'].copy()\n",
    "\n",
    "print(\"Employment status subsets created:\")\n",
    "print(f\"  Headcount - FTE: {len(headcount_filtered_fte)}, PRN: {len(headcount_filtered_prn)}\")\n",
    "print(f\"  Hires - FTE: {len(hires_filtered_fte)}, PRN: {len(hires_filtered_prn)}\")\n",
    "print(f\"  Terminations - FTE: {len(terminations_filtered_fte)}, PRN: {len(terminations_filtered_prn)}\")\n",
    "\n",
    "print(\"\\n✓ Employment status standardization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 7: Location/Facility Standardization & Classification\n",
    "\n",
    "Clean location names and classify facility types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 7: LOCATION/FACILITY STANDARDIZATION & CLASSIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def standardize_location(location):\n",
    "    \"\"\"Clean and standardize location names\"\"\"\n",
    "    if pd.isna(location):\n",
    "        return location\n",
    "    \n",
    "    # Convert to string and basic cleaning\n",
    "    loc = str(location).strip()\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    loc = re.sub(r'\\s+', ' ', loc)\n",
    "    \n",
    "    # Remove trailing punctuation and symbols\n",
    "    loc = re.sub(r'[,.\\-\\s]+{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Healthcare Workforce Analysis Template\n",
    "\n",
    "This template provides a standardized framework for analyzing healthcare workforce data across multiple health systems. It emphasizes validation, transparency, and analyst control.\n",
    "\n",
    "## Design Principles:\n",
    "- **No silent failures** - every step validates its work\n",
    "- **User controls** all mappings and parameters\n",
    "- **Consistent variable naming** for downstream use\n",
    "- **Heavy validation** with hard stops when data quality is insufficient\n",
    "- **Modular sections** that can be validated step-by-step\n",
    "\n",
    "## Workflow Sections:\n",
    "1. Data Import & Schema Setup\n",
    "2. Initial Data Exploration & Validation  \n",
    "3. Geographic Filtering\n",
    "4. Column Standardization & Mapping\n",
    "5. Job Title Standardization\n",
    "6. Employment Status Standardization\n",
    "7. Location/Facility Standardization & Classification\n",
    "8. Core Metrics Calculation\n",
    "9. Facility-Level Analysis Setup\n",
    "10. Output Generation Framework\n",
    "11. Final Validation & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from math import gcd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Data Import & Schema Setup\n",
    "\n",
    "**🔧 USER CONFIGURATION REQUIRED** \n",
    "\n",
    "Modify the configuration below to match your data sources and column structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SECTION 1: DATA IMPORT & SCHEMA SETUP\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# USER CONFIGURATION - MODIFY THESE SETTINGS\n",
    "# ==========================================\n",
    "\n",
    "# File paths and data sources\n",
    "DATA_CONFIG = {\n",
    "    'base_folder': '/path/to/your/data',  # Base folder containing data files\n",
    "    'headcount_source': {\n",
    "        'file': 'System2024.xlsx',\n",
    "        'sheet': 'Headcount',\n",
    "        'type': 'excel'  # 'excel' or 'csv'\n",
    "    },\n",
    "    'hires_source': {\n",
    "        'file': 'System2024.xlsx', \n",
    "        'sheet': 'Hires',\n",
    "        'type': 'excel'\n",
    "    },\n",
    "    'terminations_source': {\n",
    "        'file': 'System2024.xlsx',\n",
    "        'sheet': 'Terms', \n",
    "        'type': 'excel'\n",
    "    },\n",
    "    'requisitions_source': {\n",
    "        'file': 'System2024.xlsx',\n",
    "        'sheet': 'Requisitions',\n",
    "        'type': 'excel'\n",
    "    },\n",
    "    'contractors_source': {\n",
    "        'file': 'System2024.xlsx',\n",
    "        'sheet': 'Contractor',\n",
    "        'type': 'excel'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Column mapping - USER MUST SPECIFY THESE\n",
    "COLUMN_MAPPING = {\n",
    "    'job_title_col': 'Job Title',\n",
    "    'location_col': 'Location', \n",
    "    'state_col': 'Work State',\n",
    "    'full_part_time_col': 'Full/Part Time',\n",
    "    'employment_category_col': 'Employment Category',\n",
    "    'hire_date_col': 'Hire Date',  # For hires sheet\n",
    "    'term_date_col': 'Termination Date',  # For terminations sheet\n",
    "    'req_status_col': 'Req Current State',  # For requisitions sheet\n",
    "}\n",
    "\n",
    "# Analysis parameters\n",
    "ANALYSIS_CONFIG = {\n",
    "    'target_state': 'NC',  # State to filter for\n",
    "    'analysis_date': '2024-12-31',  # Date for analysis\n",
    "    'target_job_titles': ['Registered Nurse', 'Licensed Practical Nurse', 'Medical Assistant', 'Certified Nursing Assistant'],\n",
    "    'fte_values': ['FULL_TIME'],  # Values that indicate FTE status\n",
    "    'prn_values': ['PRN'],  # Values that indicate PRN status\n",
    "    'open_req_statuses': ['Not Posted', 'Posted', 'Unposted', 'In Progress'],\n",
    "    'closed_req_statuses': ['Suspended', 'Expired']\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"Target state: {ANALYSIS_CONFIG['target_state']}\")\n",
    "print(f\"Analysis date: {ANALYSIS_CONFIG['analysis_date']}\")\n",
    "print(f\"Target job titles: {ANALYSIS_CONFIG['target_job_titles']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(source_config, dataset_name):\n",
    "    \"\"\"Load a dataset based on configuration\"\"\"\n",
    "    try:\n",
    "        filepath = f\"{DATA_CONFIG['base_folder']}/{source_config['file']}\"\n",
    "        \n",
    "        if source_config['type'] == 'excel':\n",
    "            df = pd.read_excel(filepath, sheet_name=source_config['sheet'])\n",
    "            print(f\"✓ Loaded {dataset_name}: {len(df)} rows from {source_config['file']}/{source_config['sheet']}\")\n",
    "        elif source_config['type'] == 'csv':\n",
    "            df = pd.read_csv(filepath)\n",
    "            print(f\"✓ Loaded {dataset_name}: {len(df)} rows from {source_config['file']}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {source_config['type']}\")\n",
    "            \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ CRITICAL ERROR loading {dataset_name}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Load all datasets\n",
    "print(\"\\nLoading datasets...\")\n",
    "try:\n",
    "    headcount_df = load_dataset(DATA_CONFIG['headcount_source'], 'headcount')\n",
    "    hires_df = load_dataset(DATA_CONFIG['hires_source'], 'hires')\n",
    "    terminations_df = load_dataset(DATA_CONFIG['terminations_source'], 'terminations')\n",
    "    requisitions_df = load_dataset(DATA_CONFIG['requisitions_source'], 'requisitions')\n",
    "    \n",
    "    # Contractors is optional\n",
    "    try:\n",
    "        contractors_df = load_dataset(DATA_CONFIG['contractors_source'], 'contractors')\n",
    "        has_contractors = True\n",
    "    except:\n",
    "        print(\"! No contractors data found - proceeding without it\")\n",
    "        contractors_df = pd.DataFrame()\n",
    "        has_contractors = False\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ CRITICAL: Data loading failed. Cannot proceed.\")\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\n✓ Data loading complete. {4 + int(has_contractors)} datasets loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Initial Data Exploration & Validation\n",
    "\n",
    "Validate dataset structure and show data previews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 2: INITIAL DATA EXPLORATION & VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def validate_dataset_structure(df, dataset_name, required_columns):\n",
    "    \"\"\"Validate that dataset has required structure\"\"\"\n",
    "    print(f\"\\nValidating {dataset_name} structure:\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "    \n",
    "    missing_cols = []\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            missing_cols.append(col)\n",
    "            \n",
    "    if missing_cols:\n",
    "        print(f\"✗ CRITICAL: Missing required columns in {dataset_name}: {missing_cols}\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"✓ All required columns present in {dataset_name}\")\n",
    "        return True\n",
    "\n",
    "# Define required columns for each dataset\n",
    "REQUIRED_COLUMNS = {\n",
    "    'headcount': [COLUMN_MAPPING['job_title_col'], COLUMN_MAPPING['location_col'], \n",
    "                  COLUMN_MAPPING['state_col'], COLUMN_MAPPING['full_part_time_col'], \n",
    "                  COLUMN_MAPPING['employment_category_col']],\n",
    "    'hires': [COLUMN_MAPPING['job_title_col'], COLUMN_MAPPING['location_col'], \n",
    "              COLUMN_MAPPING['state_col'], COLUMN_MAPPING['hire_date_col']],\n",
    "    'terminations': [COLUMN_MAPPING['job_title_col'], COLUMN_MAPPING['location_col'], \n",
    "                     COLUMN_MAPPING['state_col'], COLUMN_MAPPING['term_date_col']],\n",
    "    'requisitions': [COLUMN_MAPPING['job_title_col'], COLUMN_MAPPING['location_col'], \n",
    "                     COLUMN_MAPPING['req_status_col']]\n",
    "}\n",
    "\n",
    "# Validate all datasets\n",
    "validation_passed = True\n",
    "datasets_to_validate = [\n",
    "    (headcount_df, 'headcount'),\n",
    "    (hires_df, 'hires'), \n",
    "    (terminations_df, 'terminations'),\n",
    "    (requisitions_df, 'requisitions')\n",
    "]\n",
    "\n",
    "if has_contractors:\n",
    "    datasets_to_validate.append((contractors_df, 'contractors'))\n",
    "\n",
    "for df, name in datasets_to_validate:\n",
    "    if name in REQUIRED_COLUMNS:\n",
    "        if not validate_dataset_structure(df, name, REQUIRED_COLUMNS[name]):\n",
    "            validation_passed = False\n",
    "\n",
    "if not validation_passed:\n",
    "    print(\"\\n✗ CRITICAL: Dataset validation failed. Check column mappings and try again.\")\n",
    "    raise ValueError(\"Dataset validation failed\")\n",
    "\n",
    "print(\"\\n✓ All datasets passed structure validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data previews\n",
    "print(\"\\nData previews:\")\n",
    "for df, name in datasets_to_validate:\n",
    "    if len(df) > 0:\n",
    "        print(f\"\\n{name.upper()} - First 3 rows:\")\n",
    "        display(df.head(3))\n",
    "        \n",
    "        # Show unique values for key columns\n",
    "        if COLUMN_MAPPING['state_col'] in df.columns:\n",
    "            states = df[COLUMN_MAPPING['state_col']].unique()\n",
    "            print(f\"  Unique states: {states}\")\n",
    "            \n",
    "        if COLUMN_MAPPING['job_title_col'] in df.columns:\n",
    "            job_titles = df[COLUMN_MAPPING['job_title_col']].value_counts().head(5)\n",
    "            print(f\"  Top 5 job titles: {dict(job_titles)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Geographic Filtering\n",
    "\n",
    "Filter all datasets for the target state and validate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 3: GEOGRAPHIC FILTERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def filter_by_state(df, dataset_name, state_col, target_state):\n",
    "    \"\"\"Filter dataset by state and validate results\"\"\"\n",
    "    original_count = len(df)\n",
    "    \n",
    "    if state_col not in df.columns:\n",
    "        print(f\"✗ CRITICAL: State column '{state_col}' not found in {dataset_name}\")\n",
    "        raise ValueError(f\"State column missing in {dataset_name}\")\n",
    "    \n",
    "    # Show unique states before filtering\n",
    "    unique_states = df[state_col].unique()\n",
    "    print(f\"\\n{dataset_name} - States before filtering: {unique_states}\")\n",
    "    \n",
    "    # Filter for target state\n",
    "    df_filtered = df[df[state_col] == target_state].copy()\n",
    "    filtered_count = len(df_filtered)\n",
    "    \n",
    "    print(f\"{dataset_name} filtering: {original_count} → {filtered_count} rows\")\n",
    "    \n",
    "    if filtered_count == 0:\n",
    "        print(f\"✗ CRITICAL: No data remains after filtering {dataset_name} for {target_state}\")\n",
    "        raise ValueError(f\"No {target_state} data in {dataset_name}\")\n",
    "    \n",
    "    # Validation - check that filtering worked\n",
    "    remaining_states = df_filtered[state_col].unique()\n",
    "    if len(remaining_states) != 1 or remaining_states[0] != target_state:\n",
    "        print(f\"✗ CRITICAL: Filtering failed for {dataset_name}. Remaining states: {remaining_states}\")\n",
    "        raise ValueError(f\"State filtering failed for {dataset_name}\")\n",
    "    \n",
    "    print(f\"✓ {dataset_name} successfully filtered to {target_state}\")\n",
    "    return df_filtered\n",
    "\n",
    "# Filter all datasets\n",
    "target_state = ANALYSIS_CONFIG['target_state']\n",
    "state_col = COLUMN_MAPPING['state_col']\n",
    "\n",
    "print(f\"Filtering all datasets for state: {target_state}\")\n",
    "\n",
    "headcount_df = filter_by_state(headcount_df, 'headcount', state_col, target_state)\n",
    "hires_df = filter_by_state(hires_df, 'hires', state_col, target_state)\n",
    "terminations_df = filter_by_state(terminations_df, 'terminations', state_col, target_state)\n",
    "requisitions_df = filter_by_state(requisitions_df, 'requisitions', state_col, target_state)\n",
    "\n",
    "if has_contractors:\n",
    "    contractors_df = filter_by_state(contractors_df, 'contractors', state_col, target_state)\n",
    "\n",
    "print(f\"\\n✓ Geographic filtering complete. All datasets filtered to {target_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Column Standardization & Mapping\n",
    "\n",
    "Apply column standardization to ensure consistent naming across datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 4: COLUMN STANDARDIZATION & MAPPING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def standardize_columns(df, dataset_name):\n",
    "    \"\"\"Apply column standardization to ensure consistent naming\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ensure all mapped columns exist and create standard names\n",
    "    column_renames = {}\n",
    "    \n",
    "    # Standard columns all datasets should have\n",
    "    if COLUMN_MAPPING['job_title_col'] in df.columns:\n",
    "        column_renames[COLUMN_MAPPING['job_title_col']] = 'Job_Title'\n",
    "    if COLUMN_MAPPING['location_col'] in df.columns:\n",
    "        column_renames[COLUMN_MAPPING['location_col']] = 'Location'\n",
    "    if COLUMN_MAPPING['state_col'] in df.columns:\n",
    "        column_renames[COLUMN_MAPPING['state_col']] = 'State'\n",
    "        \n",
    "    # Dataset-specific columns\n",
    "    if dataset_name in ['headcount', 'hires', 'terminations']:\n",
    "        if COLUMN_MAPPING['full_part_time_col'] in df.columns:\n",
    "            column_renames[COLUMN_MAPPING['full_part_time_col']] = 'Full_Part_Time'\n",
    "        if COLUMN_MAPPING['employment_category_col'] in df.columns:\n",
    "            column_renames[COLUMN_MAPPING['employment_category_col']] = 'Employment_Category'\n",
    "    \n",
    "    if dataset_name == 'hires' and COLUMN_MAPPING['hire_date_col'] in df.columns:\n",
    "        column_renames[COLUMN_MAPPING['hire_date_col']] = 'Date'\n",
    "    if dataset_name == 'terminations' and COLUMN_MAPPING['term_date_col'] in df.columns:\n",
    "        column_renames[COLUMN_MAPPING['term_date_col']] = 'Date'\n",
    "    if dataset_name == 'requisitions' and COLUMN_MAPPING['req_status_col'] in df.columns:\n",
    "        column_renames[COLUMN_MAPPING['req_status_col']] = 'Req_Status'\n",
    "    \n",
    "    # Apply renames\n",
    "    df = df.rename(columns=column_renames)\n",
    "    \n",
    "    print(f\"✓ {dataset_name} columns standardized. Renamed: {column_renames}\")\n",
    "    return df\n",
    "\n",
    "# Standardize all datasets\n",
    "print(\"Standardizing column names across all datasets...\")\n",
    "\n",
    "headcount_df = standardize_columns(headcount_df, 'headcount')\n",
    "hires_df = standardize_columns(hires_df, 'hires')\n",
    "terminations_df = standardize_columns(terminations_df, 'terminations')\n",
    "requisitions_df = standardize_columns(requisitions_df, 'requisitions')\n",
    "\n",
    "if has_contractors:\n",
    "    contractors_df = standardize_columns(contractors_df, 'contractors')\n",
    "\n",
    "print(\"✓ All datasets have standardized column names\")\n",
    "\n",
    "# Validation - ensure key columns are present\n",
    "for df, name in [(headcount_df, 'headcount'), (hires_df, 'hires'), \n",
    "                 (terminations_df, 'terminations'), (requisitions_df, 'requisitions')]:\n",
    "    if 'Job_Title' not in df.columns or 'Location' not in df.columns:\n",
    "        print(f\"✗ CRITICAL: Missing standardized columns in {name}\")\n",
    "        raise ValueError(f\"Column standardization failed for {name}\")\n",
    "\n",
    "print(\"✓ Column standardization validation passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Job Title Standardization\n",
    "\n",
    "Standardize job titles to target categories and filter for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 5: JOB TITLE STANDARDIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def standardize_job_title(title):\n",
    "    \"\"\"\n",
    "    Standardize job titles to target categories:\n",
    "    - Registered Nurse\n",
    "    - Licensed Practical Nurse  \n",
    "    - Medical Assistant\n",
    "    - Certified Nursing Assistant\n",
    "    \"\"\"\n",
    "    if pd.isna(title):\n",
    "        return None\n",
    "    \n",
    "    title_lower = str(title).lower().strip()\n",
    "    \n",
    "    # Registered Nurse variations\n",
    "    if 'rn' in title_lower or 'registered nurse' in title_lower:\n",
    "        # Exclude non-employee entries\n",
    "        if '(nonee)' in title_lower or 'traveler' in title_lower:\n",
    "            return None\n",
    "        return 'Registered Nurse'\n",
    "    \n",
    "    # Licensed Practical Nurse variations\n",
    "    elif 'lpn' in title_lower or 'licensed practical nurse' in title_lower:\n",
    "        return 'Licensed Practical Nurse'\n",
    "    \n",
    "    # Medical Assistant variations\n",
    "    elif 'medical assistant' in title_lower or 'certified medical assistant' in title_lower:\n",
    "        return 'Medical Assistant'\n",
    "    \n",
    "    # Certified Nursing Assistant variations\n",
    "    elif ('cna' in title_lower or 'certified nursing assistant' in title_lower or \n",
    "          'certified nurse assistant' in title_lower or 'nurse assistant' in title_lower or \n",
    "          'nursing assistant' in title_lower):\n",
    "        return 'Certified Nursing Assistant'\n",
    "    \n",
    "    # If none match, return None (will be filtered out)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_job_title_standardization(df, dataset_name):\n",
    "    \"\"\"Apply job title standardization and validate results\"\"\"\n",
    "    original_count = len(df)\n",
    "    \n",
    "    # Apply standardization\n",
    "    df = df.copy()\n",
    "    df['Standardized_Job_Title'] = df['Job_Title'].apply(standardize_job_title)\n",
    "    \n",
    "    # Filter for target roles only\n",
    "    df_filtered = df[df['Standardized_Job_Title'].notna()].copy()\n",
    "    filtered_count = len(df_filtered)\n",
    "    \n",
    "    # Calculate capture rate\n",
    "    capture_rate = (filtered_count / original_count * 100) if original_count > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{dataset_name} job title standardization:\")\n",
    "    print(f\"  Original rows: {original_count}\")\n",
    "    print(f\"  Rows with target job titles: {filtered_count}\")\n",
    "    print(f\"  Capture rate: {capture_rate:.1f}%\")\n",
    "    \n",
    "    # Show distribution of standardized titles\n",
    "    title_dist = df_filtered['Standardized_Job_Title'].value_counts()\n",
    "    print(f\"  Distribution: {dict(title_dist)}\")\n",
    "    \n",
    "    # Validation - ensure we have reasonable capture rate\n",
    "    if capture_rate < 10:\n",
    "        print(f\"✗ CRITICAL: Very low capture rate ({capture_rate:.1f}%) for {dataset_name}\")\n",
    "        print(\"This suggests job title standardization is not working properly.\")\n",
    "        # Show some examples of unmapped titles\n",
    "        unmapped = df[df['Standardized_Job_Title'].isna()]['Job_Title'].value_counts().head(10)\n",
    "        print(f\"Top unmapped titles: {dict(unmapped)}\")\n",
    "        raise ValueError(f\"Job title standardization failed for {dataset_name}\")\n",
    "    \n",
    "    if filtered_count == 0:\n",
    "        print(f\"✗ CRITICAL: No target job titles found in {dataset_name}\")\n",
    "        raise ValueError(f\"No target job titles in {dataset_name}\")\n",
    "    \n",
    "    print(f\"✓ {dataset_name} job title standardization successful\")\n",
    "    return df_filtered\n",
    "\n",
    "# Apply job title standardization to all datasets\n",
    "print(\"Applying job title standardization...\")\n",
    "print(f\"Target job titles: {ANALYSIS_CONFIG['target_job_titles']}\")\n",
    "\n",
    "headcount_filtered = apply_job_title_standardization(headcount_df, 'headcount')\n",
    "hires_filtered = apply_job_title_standardization(hires_df, 'hires')\n",
    "terminations_filtered = apply_job_title_standardization(terminations_df, 'terminations')\n",
    "requisitions_filtered = apply_job_title_standardization(requisitions_df, 'requisitions')\n",
    "\n",
    "if has_contractors:\n",
    "    contractors_filtered = apply_job_title_standardization(contractors_df, 'contractors')\n",
    "\n",
    "print(\"\\n✓ Job title standardization complete for all datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-dataset validation - ensure we have consistent job titles\n",
    "all_job_titles = set()\n",
    "for df, name in [(headcount_filtered, 'headcount'), (hires_filtered, 'hires'),\n",
    "                 (terminations_filtered, 'terminations'), (requisitions_filtered, 'requisitions')]:\n",
    "    titles = set(df['Standardized_Job_Title'].unique())\n",
    "    all_job_titles.update(titles)\n",
    "    \n",
    "expected_titles = set(ANALYSIS_CONFIG['target_job_titles'])
if not all_job_titles.issubset(expected_titles):
    print(f"✗ WARNING: Unexpected job titles found: {all_job_titles - expected_titles}")

print(f"✓ Job title validation complete. Found titles: {sorted(all_job_titles)}"), '', loc)\n",
    "    \n",
    "    # Standardize common abbreviations\n",
    "    loc = re.sub(r'\\bSt\\b', 'St', loc)  # Standardize Street\n",
    "    loc = re.sub(r'\\bDr\\b', 'Dr', loc)  # Standardize Drive\n",
    "    loc = re.sub(r'\\bSte\\b', 'Suite', loc)  # Standardize Suite\n",
    "    \n",
    "    # Fix state abbreviations (add comma before state)\n",
    "    loc = re.sub(r'\\s+(NC|SC|VA|TN|GA)\\s*{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Healthcare Workforce Analysis Template\n",
    "\n",
    "This template provides a standardized framework for analyzing healthcare workforce data across multiple health systems. It emphasizes validation, transparency, and analyst control.\n",
    "\n",
    "## Design Principles:\n",
    "- **No silent failures** - every step validates its work\n",
    "- **User controls** all mappings and parameters\n",
    "- **Consistent variable naming** for downstream use\n",
    "- **Heavy validation** with hard stops when data quality is insufficient\n",
    "- **Modular sections** that can be validated step-by-step\n",
    "\n",
    "## Workflow Sections:\n",
    "1. Data Import & Schema Setup\n",
    "2. Initial Data Exploration & Validation  \n",
    "3. Geographic Filtering\n",
    "4. Column Standardization & Mapping\n",
    "5. Job Title Standardization\n",
    "6. Employment Status Standardization\n",
    "7. Location/Facility Standardization & Classification\n",
    "8. Core Metrics Calculation\n",
    "9. Facility-Level Analysis Setup\n",
    "10. Output Generation Framework\n",
    "11. Final Validation & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from math import gcd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Data Import & Schema Setup\n",
    "\n",
    "**🔧 USER CONFIGURATION REQUIRED** \n",
    "\n",
    "Modify the configuration below to match your data sources and column structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SECTION 1: DATA IMPORT & SCHEMA SETUP\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# USER CONFIGURATION - MODIFY THESE SETTINGS\n",
    "# ==========================================\n",
    "\n",
    "# File paths and data sources\n",
    "DATA_CONFIG = {\n",
    "    'base_folder': '/path/to/your/data',  # Base folder containing data files\n",
    "    'headcount_source': {\n",
    "        'file': 'System2024.xlsx',\n",
    "        'sheet': 'Headcount',\n",
    "        'type': 'excel'  # 'excel' or 'csv'\n",
    "    },\n",
    "    'hires_source': {\n",
    "        'file': 'System2024.xlsx', \n",
    "        'sheet': 'Hires',\n",
    "        'type': 'excel'\n",
    "    },\n",
    "    'terminations_source': {\n",
    "        'file': 'System2024.xlsx',\n",
    "        'sheet': 'Terms', \n",
    "        'type': 'excel'\n",
    "    },\n",
    "    'requisitions_source': {\n",
    "        'file': 'System2024.xlsx',\n",
    "        'sheet': 'Requisitions',\n",
    "        'type': 'excel'\n",
    "    },\n",
    "    'contractors_source': {\n",
    "        'file': 'System2024.xlsx',\n",
    "        'sheet': 'Contractor',\n",
    "        'type': 'excel'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Column mapping - USER MUST SPECIFY THESE\n",
    "COLUMN_MAPPING = {\n",
    "    'job_title_col': 'Job Title',\n",
    "    'location_col': 'Location', \n",
    "    'state_col': 'Work State',\n",
    "    'full_part_time_col': 'Full/Part Time',\n",
    "    'employment_category_col': 'Employment Category',\n",
    "    'hire_date_col': 'Hire Date',  # For hires sheet\n",
    "    'term_date_col': 'Termination Date',  # For terminations sheet\n",
    "    'req_status_col': 'Req Current State',  # For requisitions sheet\n",
    "}\n",
    "\n",
    "# Analysis parameters\n",
    "ANALYSIS_CONFIG = {\n",
    "    'target_state': 'NC',  # State to filter for\n",
    "    'analysis_date': '2024-12-31',  # Date for analysis\n",
    "    'target_job_titles': ['Registered Nurse', 'Licensed Practical Nurse', 'Medical Assistant', 'Certified Nursing Assistant'],\n",
    "    'fte_values': ['FULL_TIME'],  # Values that indicate FTE status\n",
    "    'prn_values': ['PRN'],  # Values that indicate PRN status\n",
    "    'open_req_statuses': ['Not Posted', 'Posted', 'Unposted', 'In Progress'],\n",
    "    'closed_req_statuses': ['Suspended', 'Expired']\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"Target state: {ANALYSIS_CONFIG['target_state']}\")\n",
    "print(f\"Analysis date: {ANALYSIS_CONFIG['analysis_date']}\")\n",
    "print(f\"Target job titles: {ANALYSIS_CONFIG['target_job_titles']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(source_config, dataset_name):\n",
    "    \"\"\"Load a dataset based on configuration\"\"\"\n",
    "    try:\n",
    "        filepath = f\"{DATA_CONFIG['base_folder']}/{source_config['file']}\"\n",
    "        \n",
    "        if source_config['type'] == 'excel':\n",
    "            df = pd.read_excel(filepath, sheet_name=source_config['sheet'])\n",
    "            print(f\"✓ Loaded {dataset_name}: {len(df)} rows from {source_config['file']}/{source_config['sheet']}\")\n",
    "        elif source_config['type'] == 'csv':\n",
    "            df = pd.read_csv(filepath)\n",
    "            print(f\"✓ Loaded {dataset_name}: {len(df)} rows from {source_config['file']}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {source_config['type']}\")\n",
    "            \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ CRITICAL ERROR loading {dataset_name}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Load all datasets\n",
    "print(\"\\nLoading datasets...\")\n",
    "try:\n",
    "    headcount_df = load_dataset(DATA_CONFIG['headcount_source'], 'headcount')\n",
    "    hires_df = load_dataset(DATA_CONFIG['hires_source'], 'hires')\n",
    "    terminations_df = load_dataset(DATA_CONFIG['terminations_source'], 'terminations')\n",
    "    requisitions_df = load_dataset(DATA_CONFIG['requisitions_source'], 'requisitions')\n",
    "    \n",
    "    # Contractors is optional\n",
    "    try:\n",
    "        contractors_df = load_dataset(DATA_CONFIG['contractors_source'], 'contractors')\n",
    "        has_contractors = True\n",
    "    except:\n",
    "        print(\"! No contractors data found - proceeding without it\")\n",
    "        contractors_df = pd.DataFrame()\n",
    "        has_contractors = False\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ CRITICAL: Data loading failed. Cannot proceed.\")\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\n✓ Data loading complete. {4 + int(has_contractors)} datasets loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Initial Data Exploration & Validation\n",
    "\n",
    "Validate dataset structure and show data previews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 2: INITIAL DATA EXPLORATION & VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def validate_dataset_structure(df, dataset_name, required_columns):\n",
    "    \"\"\"Validate that dataset has required structure\"\"\"\n",
    "    print(f\"\\nValidating {dataset_name} structure:\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "    \n",
    "    missing_cols = []\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            missing_cols.append(col)\n",
    "            \n",
    "    if missing_cols:\n",
    "        print(f\"✗ CRITICAL: Missing required columns in {dataset_name}: {missing_cols}\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"✓ All required columns present in {dataset_name}\")\n",
    "        return True\n",
    "\n",
    "# Define required columns for each dataset\n",
    "REQUIRED_COLUMNS = {\n",
    "    'headcount': [COLUMN_MAPPING['job_title_col'], COLUMN_MAPPING['location_col'], \n",
    "                  COLUMN_MAPPING['state_col'], COLUMN_MAPPING['full_part_time_col'], \n",
    "                  COLUMN_MAPPING['employment_category_col']],\n",
    "    'hires': [COLUMN_MAPPING['job_title_col'], COLUMN_MAPPING['location_col'], \n",
    "              COLUMN_MAPPING['state_col'], COLUMN_MAPPING['hire_date_col']],\n",
    "    'terminations': [COLUMN_MAPPING['job_title_col'], COLUMN_MAPPING['location_col'], \n",
    "                     COLUMN_MAPPING['state_col'], COLUMN_MAPPING['term_date_col']],\n",
    "    'requisitions': [COLUMN_MAPPING['job_title_col'], COLUMN_MAPPING['location_col'], \n",
    "                     COLUMN_MAPPING['req_status_col']]\n",
    "}\n",
    "\n",
    "# Validate all datasets\n",
    "validation_passed = True\n",
    "datasets_to_validate = [\n",
    "    (headcount_df, 'headcount'),\n",
    "    (hires_df, 'hires'), \n",
    "    (terminations_df, 'terminations'),\n",
    "    (requisitions_df, 'requisitions')\n",
    "]\n",
    "\n",
    "if has_contractors:\n",
    "    datasets_to_validate.append((contractors_df, 'contractors'))\n",
    "\n",
    "for df, name in datasets_to_validate:\n",
    "    if name in REQUIRED_COLUMNS:\n",
    "        if not validate_dataset_structure(df, name, REQUIRED_COLUMNS[name]):\n",
    "            validation_passed = False\n",
    "\n",
    "if not validation_passed:\n",
    "    print(\"\\n✗ CRITICAL: Dataset validation failed. Check column mappings and try again.\")\n",
    "    raise ValueError(\"Dataset validation failed\")\n",
    "\n",
    "print(\"\\n✓ All datasets passed structure validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data previews\n",
    "print(\"\\nData previews:\")\n",
    "for df, name in datasets_to_validate:\n",
    "    if len(df) > 0:\n",
    "        print(f\"\\n{name.upper()} - First 3 rows:\")\n",
    "        display(df.head(3))\n",
    "        \n",
    "        # Show unique values for key columns\n",
    "        if COLUMN_MAPPING['state_col'] in df.columns:\n",
    "            states = df[COLUMN_MAPPING['state_col']].unique()\n",
    "            print(f\"  Unique states: {states}\")\n",
    "            \n",
    "        if COLUMN_MAPPING['job_title_col'] in df.columns:\n",
    "            job_titles = df[COLUMN_MAPPING['job_title_col']].value_counts().head(5)\n",
    "            print(f\"  Top 5 job titles: {dict(job_titles)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Geographic Filtering\n",
    "\n",
    "Filter all datasets for the target state and validate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 3: GEOGRAPHIC FILTERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def filter_by_state(df, dataset_name, state_col, target_state):\n",
    "    \"\"\"Filter dataset by state and validate results\"\"\"\n",
    "    original_count = len(df)\n",
    "    \n",
    "    if state_col not in df.columns:\n",
    "        print(f\"✗ CRITICAL: State column '{state_col}' not found in {dataset_name}\")\n",
    "        raise ValueError(f\"State column missing in {dataset_name}\")\n",
    "    \n",
    "    # Show unique states before filtering\n",
    "    unique_states = df[state_col].unique()\n",
    "    print(f\"\\n{dataset_name} - States before filtering: {unique_states}\")\n",
    "    \n",
    "    # Filter for target state\n",
    "    df_filtered = df[df[state_col] == target_state].copy()\n",
    "    filtered_count = len(df_filtered)\n",
    "    \n",
    "    print(f\"{dataset_name} filtering: {original_count} → {filtered_count} rows\")\n",
    "    \n",
    "    if filtered_count == 0:\n",
    "        print(f\"✗ CRITICAL: No data remains after filtering {dataset_name} for {target_state}\")\n",
    "        raise ValueError(f\"No {target_state} data in {dataset_name}\")\n",
    "    \n",
    "    # Validation - check that filtering worked\n",
    "    remaining_states = df_filtered[state_col].unique()\n",
    "    if len(remaining_states) != 1 or remaining_states[0] != target_state:\n",
    "        print(f\"✗ CRITICAL: Filtering failed for {dataset_name}. Remaining states: {remaining_states}\")\n",
    "        raise ValueError(f\"State filtering failed for {dataset_name}\")\n",
    "    \n",
    "    print(f\"✓ {dataset_name} successfully filtered to {target_state}\")\n",
    "    return df_filtered\n",
    "\n",
    "# Filter all datasets\n",
    "target_state = ANALYSIS_CONFIG['target_state']\n",
    "state_col = COLUMN_MAPPING['state_col']\n",
    "\n",
    "print(f\"Filtering all datasets for state: {target_state}\")\n",
    "\n",
    "headcount_df = filter_by_state(headcount_df, 'headcount', state_col, target_state)\n",
    "hires_df = filter_by_state(hires_df, 'hires', state_col, target_state)\n",
    "terminations_df = filter_by_state(terminations_df, 'terminations', state_col, target_state)\n",
    "requisitions_df = filter_by_state(requisitions_df, 'requisitions', state_col, target_state)\n",
    "\n",
    "if has_contractors:\n",
    "    contractors_df = filter_by_state(contractors_df, 'contractors', state_col, target_state)\n",
    "\n",
    "print(f\"\\n✓ Geographic filtering complete. All datasets filtered to {target_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Column Standardization & Mapping\n",
    "\n",
    "Apply column standardization to ensure consistent naming across datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 4: COLUMN STANDARDIZATION & MAPPING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def standardize_columns(df, dataset_name):\n",
    "    \"\"\"Apply column standardization to ensure consistent naming\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ensure all mapped columns exist and create standard names\n",
    "    column_renames = {}\n",
    "    \n",
    "    # Standard columns all datasets should have\n",
    "    if COLUMN_MAPPING['job_title_col'] in df.columns:\n",
    "        column_renames[COLUMN_MAPPING['job_title_col']] = 'Job_Title'\n",
    "    if COLUMN_MAPPING['location_col'] in df.columns:\n",
    "        column_renames[COLUMN_MAPPING['location_col']] = 'Location'\n",
    "    if COLUMN_MAPPING['state_col'] in df.columns:\n",
    "        column_renames[COLUMN_MAPPING['state_col']] = 'State'\n",
    "        \n",
    "    # Dataset-specific columns\n",
    "    if dataset_name in ['headcount', 'hires', 'terminations']:\n",
    "        if COLUMN_MAPPING['full_part_time_col'] in df.columns:\n",
    "            column_renames[COLUMN_MAPPING['full_part_time_col']] = 'Full_Part_Time'\n",
    "        if COLUMN_MAPPING['employment_category_col'] in df.columns:\n",
    "            column_renames[COLUMN_MAPPING['employment_category_col']] = 'Employment_Category'\n",
    "    \n",
    "    if dataset_name == 'hires' and COLUMN_MAPPING['hire_date_col'] in df.columns:\n",
    "        column_renames[COLUMN_MAPPING['hire_date_col']] = 'Date'\n",
    "    if dataset_name == 'terminations' and COLUMN_MAPPING['term_date_col'] in df.columns:\n",
    "        column_renames[COLUMN_MAPPING['term_date_col']] = 'Date'\n",
    "    if dataset_name == 'requisitions' and COLUMN_MAPPING['req_status_col'] in df.columns:\n",
    "        column_renames[COLUMN_MAPPING['req_status_col']] = 'Req_Status'\n",
    "    \n",
    "    # Apply renames\n",
    "    df = df.rename(columns=column_renames)\n",
    "    \n",
    "    print(f\"✓ {dataset_name} columns standardized. Renamed: {column_renames}\")\n",
    "    return df\n",
    "\n",
    "# Standardize all datasets\n",
    "print(\"Standardizing column names across all datasets...\")\n",
    "\n",
    "headcount_df = standardize_columns(headcount_df, 'headcount')\n",
    "hires_df = standardize_columns(hires_df, 'hires')\n",
    "terminations_df = standardize_columns(terminations_df, 'terminations')\n",
    "requisitions_df = standardize_columns(requisitions_df, 'requisitions')\n",
    "\n",
    "if has_contractors:\n",
    "    contractors_df = standardize_columns(contractors_df, 'contractors')\n",
    "\n",
    "print(\"✓ All datasets have standardized column names\")\n",
    "\n",
    "# Validation - ensure key columns are present\n",
    "for df, name in [(headcount_df, 'headcount'), (hires_df, 'hires'), \n",
    "                 (terminations_df, 'terminations'), (requisitions_df, 'requisitions')]:\n",
    "    if 'Job_Title' not in df.columns or 'Location' not in df.columns:\n",
    "        print(f\"✗ CRITICAL: Missing standardized columns in {name}\")\n",
    "        raise ValueError(f\"Column standardization failed for {name}\")\n",
    "\n",
    "print(\"✓ Column standardization validation passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Job Title Standardization\n",
    "\n",
    "Standardize job titles to target categories and filter for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 5: JOB TITLE STANDARDIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def standardize_job_title(title):\n",
    "    \"\"\"\n",
    "    Standardize job titles to target categories:\n",
    "    - Registered Nurse\n",
    "    - Licensed Practical Nurse  \n",
    "    - Medical Assistant\n",
    "    - Certified Nursing Assistant\n",
    "    \"\"\"\n",
    "    if pd.isna(title):\n",
    "        return None\n",
    "    \n",
    "    title_lower = str(title).lower().strip()\n",
    "    \n",
    "    # Registered Nurse variations\n",
    "    if 'rn' in title_lower or 'registered nurse' in title_lower:\n",
    "        # Exclude non-employee entries\n",
    "        if '(nonee)' in title_lower or 'traveler' in title_lower:\n",
    "            return None\n",
    "        return 'Registered Nurse'\n",
    "    \n",
    "    # Licensed Practical Nurse variations\n",
    "    elif 'lpn' in title_lower or 'licensed practical nurse' in title_lower:\n",
    "        return 'Licensed Practical Nurse'\n",
    "    \n",
    "    # Medical Assistant variations\n",
    "    elif 'medical assistant' in title_lower or 'certified medical assistant' in title_lower:\n",
    "        return 'Medical Assistant'\n",
    "    \n",
    "    # Certified Nursing Assistant variations\n",
    "    elif ('cna' in title_lower or 'certified nursing assistant' in title_lower or \n",
    "          'certified nurse assistant' in title_lower or 'nurse assistant' in title_lower or \n",
    "          'nursing assistant' in title_lower):\n",
    "        return 'Certified Nursing Assistant'\n",
    "    \n",
    "    # If none match, return None (will be filtered out)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_job_title_standardization(df, dataset_name):\n",
    "    \"\"\"Apply job title standardization and validate results\"\"\"\n",
    "    original_count = len(df)\n",
    "    \n",
    "    # Apply standardization\n",
    "    df = df.copy()\n",
    "    df['Standardized_Job_Title'] = df['Job_Title'].apply(standardize_job_title)\n",
    "    \n",
    "    # Filter for target roles only\n",
    "    df_filtered = df[df['Standardized_Job_Title'].notna()].copy()\n",
    "    filtered_count = len(df_filtered)\n",
    "    \n",
    "    # Calculate capture rate\n",
    "    capture_rate = (filtered_count / original_count * 100) if original_count > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{dataset_name} job title standardization:\")\n",
    "    print(f\"  Original rows: {original_count}\")\n",
    "    print(f\"  Rows with target job titles: {filtered_count}\")\n",
    "    print(f\"  Capture rate: {capture_rate:.1f}%\")\n",
    "    \n",
    "    # Show distribution of standardized titles\n",
    "    title_dist = df_filtered['Standardized_Job_Title'].value_counts()\n",
    "    print(f\"  Distribution: {dict(title_dist)}\")\n",
    "    \n",
    "    # Validation - ensure we have reasonable capture rate\n",
    "    if capture_rate < 10:\n",
    "        print(f\"✗ CRITICAL: Very low capture rate ({capture_rate:.1f}%) for {dataset_name}\")\n",
    "        print(\"This suggests job title standardization is not working properly.\")\n",
    "        # Show some examples of unmapped titles\n",
    "        unmapped = df[df['Standardized_Job_Title'].isna()]['Job_Title'].value_counts().head(10)\n",
    "        print(f\"Top unmapped titles: {dict(unmapped)}\")\n",
    "        raise ValueError(f\"Job title standardization failed for {dataset_name}\")\n",
    "    \n",
    "    if filtered_count == 0:\n",
    "        print(f\"✗ CRITICAL: No target job titles found in {dataset_name}\")\n",
    "        raise ValueError(f\"No target job titles in {dataset_name}\")\n",
    "    \n",
    "    print(f\"✓ {dataset_name} job title standardization successful\")\n",
    "    return df_filtered\n",
    "\n",
    "# Apply job title standardization to all datasets\n",
    "print(\"Applying job title standardization...\")\n",
    "print(f\"Target job titles: {ANALYSIS_CONFIG['target_job_titles']}\")\n",
    "\n",
    "headcount_filtered = apply_job_title_standardization(headcount_df, 'headcount')\n",
    "hires_filtered = apply_job_title_standardization(hires_df, 'hires')\n",
    "terminations_filtered = apply_job_title_standardization(terminations_df, 'terminations')\n",
    "requisitions_filtered = apply_job_title_standardization(requisitions_df, 'requisitions')\n",
    "\n",
    "if has_contractors:\n",
    "    contractors_filtered = apply_job_title_standardization(contractors_df, 'contractors')\n",
    "\n",
    "print(\"\\n✓ Job title standardization complete for all datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-dataset validation - ensure we have consistent job titles\n",
    "all_job_titles = set()\n",
    "for df, name in [(headcount_filtered, 'headcount'), (hires_filtered, 'hires'),\n",
    "                 (terminations_filtered, 'terminations'), (requisitions_filtered, 'requisitions')]:\n",
    "    titles = set(df['Standardized_Job_Title'].unique())\n",
    "    all_job_titles.update(titles)\n",
    "    \n",
    "expected_titles = set(ANALYSIS_CONFIG['target_job_titles'])
if not all_job_titles.issubset(expected_titles):
    print(f"✗ WARNING: Unexpected job titles found: {all_job_titles - expected_titles}")

print(f"✓ Job title validation complete. Found titles: {sorted(all_job_titles)}"), r', \\1', loc)\n",
    "    \n",
    "    return loc.strip()\n",
    "\n",
    "def classify_facility_type_detailed(facility_name):\n",
    "    \"\"\"\n",
    "    Classify facility type and sub-type based on keywords in facility name\n",
    "    Returns tuple: (primary_type, sub_type)\n",
    "    \"\"\"\n",
    "    if pd.isna(facility_name):\n",
    "        return 'Unknown', 'Unknown'\n",
    "    \n",
    "    name_lower = str(facility_name).lower()\n",
    "    \n",
    "    # Hospital classification\n",
    "    if any(keyword in name_lower for keyword in [\n",
    "        'hospital', 'medical center', 'medical centre', 'regional medical', \n",
    "        'health system', 'emergency', 'trauma', 'intensive care', 'icu',\n",
    "        'surgery center', 'surgical center', 'ambulatory surgery'\n",
    "    ]):\n",
    "        # Sub-type classification for hospitals\n",
    "        if any(keyword in name_lower for keyword in ['children', 'pediatric', 'peds']):\n",
    "            sub_type = 'Pediatric Hospital'\n",
    "        elif any(keyword in name_lower for keyword in ['regional', 'main', 'flagship']):\n",
    "            sub_type = 'Regional Hospital'\n",
    "        elif any(keyword in name_lower for keyword in ['community', 'local']):\n",
    "            sub_type = 'Community Hospital'\n",
    "        elif any(keyword in name_lower for keyword in ['specialty', 'surgical', 'surgery']):\n",
    "            sub_type = 'Specialty Hospital'\n",
    "        else:\n",
    "            sub_type = 'General Hospital'\n",
    "        \n",
    "        return 'Hospital', sub_type\n",
    "    \n",
    "    # Clinic classification\n",
    "    elif any(keyword in name_lower for keyword in [\n",
    "        'clinic', 'medical group', 'family medicine', 'primary care', 'urgent care',\n",
    "        'outpatient', 'ambulatory', 'practice', 'office', 'center', 'specialty',\n",
    "        'cardiology', 'orthopedic', 'oncology', 'cancer', 'heart', 'diabetes',\n",
    "        'rehabilitation', 'physical therapy', 'occupational health', 'employee health',\n",
    "        'fracture', 'ent', 'bariatrics', 'imaging', 'gynecology', 'endoscopy',\n",
    "        'physician', 'pulmonary', 'endocrine', 'surgery', 'surg', 'weight', 'pain',\n",
    "        'walk-in', 'womancare', 'pulmonology', 'rheumatology', 'arthritis', 'spine',\n",
    "        'vascular', 'hyperbarics', 'healthcare', 'critical care', 'infectious',\n",
    "        'neuroscience', 'women\\'s', 'sports', 'mobile', 'neuro'\n",
    "    ]):\n",
    "        # Sub-type classification for clinics\n",
    "        if any(keyword in name_lower for keyword in ['dental', 'dentist']):\n",
    "            sub_type = 'Dental'\n",
    "        elif any(keyword in name_lower for keyword in ['eye', 'vision', 'ophthalmology', 'optometry']):\n",
    "            sub_type = 'Eye Care'\n",
    "        elif any(keyword in name_lower for keyword in ['urgent care', 'walk-in']):\n",
    "            sub_type = 'Urgent Care'\n",
    "        elif any(keyword in name_lower for keyword in ['primary care', 'family medicine']):\n",
    "            sub_type = 'Primary Care'\n",
    "        else:\n",
    "            sub_type = 'Specialty Clinic'\n",
    "        \n",
    "        return 'Clinic', sub_type\n",
    "    \n",
    "    # Default fallback\n",
    "    else:\n",
    "        return 'Other', 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_location_standardization(datasets_dict):\n",
    "    \"\"\"Apply location standardization and facility classification to all datasets\"\"\"\n",
    "    print(\"Standardizing locations across all datasets...\")\n",
    "    \n",
    "    # Collect all unique locations\n",
    "    all_locations = set()\n",
    "    for name, df in datasets_dict.items():\n",
    "        if 'Location' in df.columns:\n",
    "            locations = df['Location'].dropna().unique()\n",
    "            all_locations.update(locations)\n",
    "    \n",
    "    original_location_count = len(all_locations)\n",
    "    print(f\"Found {original_location_count} unique locations across all datasets\")\n",
    "    \n",
    "    # Apply location standardization to each dataset\n",
    "    for name, df in datasets_dict.items():\n",
    "        if 'Location' in df.columns:\n",
    "            print(f\"  Standardizing locations in {name}...\")\n",
    "            df['Location'] = df['Location'].apply(standardize_location)\n",
    "    \n",
    "    # Re-collect locations after standardization\n",
    "    all_locations_clean = set()\n",
    "    for name, df in datasets_dict.items():\n",
    "        if 'Location' in df.columns:\n",
    "            locations = df['Location'].dropna().unique()\n",
    "            all_locations_clean.update(locations)\n",
    "    \n",
    "    cleaned_location_count = len(all_locations_clean)\n",
    "    print(f\"After standardization: {cleaned_location_count} unique locations\")\n",
    "    print(f\"Consolidation: {original_location_count - cleaned_location_count} duplicates removed\")\n",
    "    \n",
    "    # Apply facility type classification\n",
    "    print(\"\\nApplying facility type classification...\")\n",
    "    \n",
    "    # Create facility type lookup\n",
    "    unique_facilities = pd.DataFrame({'Facility_Name': sorted(all_locations_clean)})\n",
    "    facility_classifications = unique_facilities['Facility_Name'].apply(classify_facility_type_detailed)\n",
    "    unique_facilities['Facility_Type'] = [classification[0] for classification in facility_classifications]\n",
    "    unique_facilities['Facility_Sub_Type'] = [classification[1] for classification in facility_classifications]\n",
    "    \n",
    "    # Show facility type distribution\n",
    "    print(\"\\nFacility type distribution:\")\n",
    "    type_dist = unique_facilities['Facility_Type'].value_counts()\n",
    "    for facility_type, count in type_dist.items():\n",
    "        print(f\"  {facility_type}: {count} facilities\")\n",
    "        \n",
    "        # Show sub-types for each primary type\n",
    "        sub_types = unique_facilities[unique_facilities['Facility_Type'] == facility_type]['Facility_Sub_Type'].value_counts()\n",
    "        for sub_type, sub_count in sub_types.items():\n",
    "            print(f\"    {sub_type}: {sub_count}\")\n",
    "    \n",
    "    # Add facility types to all datasets\n",
    "    facility_lookup = unique_facilities.set_index('Facility_Name')[['Facility_Type', 'Facility_Sub_Type']]\n",
    "    \n",
    "    for name, df in datasets_dict.items():\n",
    "        if 'Location' in df.columns:\n",
    "            print(f\"  Adding facility types to {name}...\")\n",
    "            df['Facility_Type'] = df['Location'].map(facility_lookup['Facility_Type'])\n",
    "            df['Facility_Sub_Type'] = df['Location'].map(facility_lookup['Facility_Sub_Type'])\n",
    "            \n",
    "            # Validation - ensure all locations got classified\n",
    "            unclassified = df['Facility_Type'].isna().sum()\n",
    "            if unclassified > 0:\n",
    "                print(f\"✗ WARNING: {unclassified} locations in {name} could not be classified\")\n",
    "                unclassified_locs = df[df['Facility_Type'].isna()]['Location'].unique()[:5]\n",
    "                print(f\"Examples: {list(unclassified_locs)}\")\n",
    "    \n",
    "    print(\"✓ Location standardization and facility classification complete\")\n",
    "    return datasets_dict\n",
    "\n",
    "# Apply location standardization\n",
    "datasets_for_location = {\n",
    "    'headcount_filtered': headcount_filtered,\n",
    "    'hires_filtered': hires_filtered,\n",
    "    'terminations_filtered': terminations_filtered,\n",
    "    'requisitions_filtered': requisitions_filtered\n",
    "}\n",
    "\n",
    "if has_contractors:\n",
    "    datasets_for_location['contractors_filtered'] = contractors_filtered\n",
    "\n",
    "datasets_for_location = apply_location_standardization(datasets_for_location)\n",
    "\n",
    "# Update dataset variables\n",
    "headcount_filtered = datasets_for_location['headcount_filtered']\n",
    "hires_filtered = datasets_for_location['hires_filtered']\n",
    "terminations_filtered = datasets_for_location['terminations_filtered']\n",
    "requisitions_filtered = datasets_for_location['requisitions_filtered']\n",
    "\n",
    "if has_contractors:\n",
    "    contractors_filtered = datasets_for_location['contractors_filtered']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 8: Core Metrics Calculation\n",
    "\n",
    "Calculate system-wide and job title-level workforce metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 8: CORE METRICS CALCULATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def calculate_system_wide_metrics():\n",
    "    \"\"\"Calculate system-wide workforce metrics\"\"\"\n",
    "    print(\"Calculating system-wide metrics...\")\n",
    "    \n",
    "    # Basic counts (FTE only for primary metrics)\n",
    "    total_headcount_fte = len(headcount_filtered_fte)\n",
    "    total_hires_fte = len(hires_filtered_fte)\n",
    "    total_terminations_fte = len(terminations_filtered_fte)\n",
    "    \n",
    "    # Contingent workforce counts\n",
    "    total_headcount_prn = len(headcount_filtered_prn)\n",
    "    total_hires_prn = len(hires_filtered_prn)\n",
    "    total_terminations_prn = len(terminations_filtered_prn)\n",
    "    \n",
    "    # All workforce counts\n",
    "    total_headcount_all = len(headcount_filtered)\n",
    "    total_hires_all = len(hires_filtered)\n",
    "    total_terminations_all = len(terminations_filtered)\n",
    "    \n",
    "    # Facility counts\n",
    "    all_facilities = pd.concat([\n",
    "        headcount_filtered['Location'],\n",
    "        hires_filtered['Location'], \n",
    "        terminations_filtered['Location']\n",
    "    ]).drop_duplicates()\n",
    "    num_facilities = all_facilities.nunique()\n",
    "    \n",
    "    # Requisitions analysis\n",
    "    open_requisitions = 0\n",
    "    closed_requisitions = 0\n",
    "    total_requisitions = len(requisitions_filtered)\n",
    "    \n",
    "    if 'Req_Status' in requisitions_filtered.columns:\n",
    "        open_statuses = ANALYSIS_CONFIG['open_req_statuses']\n",
    "        closed_statuses = ANALYSIS_CONFIG['closed_req_statuses']\n",
    "        \n",
    "        open_requisitions = requisitions_filtered['Req_Status'].isin(open_statuses).sum()\n",
    "        closed_requisitions = requisitions_filtered['Req_Status'].isin(closed_statuses).sum()\n",
    "    \n",
    "    # Calculate derived metrics\n",
    "    net_change_fte = total_hires_fte - total_terminations_fte\n",
    "    churn_rate_fte = (total_terminations_fte / total_headcount_fte * 100) if total_headcount_fte > 0 else 0\n",
    "    vacancy_rate = (open_requisitions / total_headcount_all * 100) if total_headcount_all > 0 else 0\n",
    "    \n",
    "    # Contingent to FTE ratio\n",
    "    if total_headcount_fte > 0:\n",
    "        ratio_prn = total_headcount_prn\n",
    "        ratio_fte = total_headcount_fte\n",
    "        common_divisor = gcd(ratio_prn, ratio_fte) if ratio_prn > 0 else 1\n",
    "        ratio_prn_simplified = ratio_prn // common_divisor if ratio_prn > 0 else 0\n",
    "        ratio_fte_simplified = ratio_fte // common_divisor\n",
    "        contingent_to_fte_ratio = f\"{ratio_prn_simplified}:{ratio_fte_simplified}\"\n",
    "    else:\n",
    "        contingent_to_fte_ratio = \"N/A\"\n",
    "    \n",
    "    # Create system overview\n",
    "    system_overview = {\n",
    "        'Analysis_Date': ANALYSIS_CONFIG['analysis_date'],\n",
    "        'Target_State': ANALYSIS_CONFIG['target_state'],\n",
    "        'Total_Headcount_FTE': total_headcount_fte,\n",
    "        'Total_Headcount_PRN': total_headcount_prn,\n",
    "        'Total_Headcount_All': total_headcount_all,\n",
    "        'Total_Hires_FTE': total_hires_fte,\n",
    "        'Total_Hires_PRN': total_hires_prn,\n",
    "        'Total_Hires_All': total_hires_all,\n",
    "        'Total_Terminations_FTE': total_terminations_fte,\n",
    "        'Total_Terminations_PRN': total_terminations_prn,\n",
    "        'Total_Terminations_All': total_terminations_all,\n",
    "        'Net_Change_FTE': net_change_fte,\n",
    "        'Open_Requisitions': open_requisitions,\n",
    "        'Closed_Requisitions': closed_requisitions,\n",
    "        'Total_Requisitions': total_requisitions,\n",
    "        'Vacancy_Rate_Percent': round(vacancy_rate, 1),\n",
    "        'Churn_Rate_FTE_Percent': round(churn_rate_fte, 1),\n",
    "        'Contingent_to_FTE_Ratio': contingent_to_fte_ratio,\n",
    "        'Number_of_Facilities': num_facilities\n",
    "    }\n",
    "    \n",
    "    print(\"SYSTEM-WIDE METRICS:\")\n",
    "    for key, value in system_overview.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Validation checks\n",
    "    print(f\"\\nVALIDATION CHECKS:\")\n",
    "    print(f\"  Total headcount matches: {total_headcount_fte + total_headcount_prn == total_headcount_all}\")\n",
    "    print(f\"  Total hires matches: {total_hires_fte + total_hires_prn == total_hires_all}\")\n",
    "    print(f\"  Total terminations matches: {total_terminations_fte + total_terminations_prn == total_terminations_all}\")\n",
    "    print(f\"  Requisitions categorized: {open_requisitions + closed_requisitions} of {total_requisitions}\")\n",
    "    \n",
    "    return system_overview\n",
    "\n",
    "# Calculate system-wide metrics\n",
    "system_overview = calculate_system_wide_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_job_title_metrics():\n",
    "    \"\"\"Calculate metrics by standardized job title\"\"\"\n",
    "    print(\"\\nCalculating metrics by job title...\")\n",
    "    \n",
    "    # Headcount by job title\n",
    "    headcount_by_role = headcount_filtered.groupby('Standardized_Job_Title').size().rename('Headcount_All')\n",
    "    headcount_fte_by_role = headcount_filtered_fte.groupby('Standardized_Job_Title').size().rename('Headcount_FTE')\n",
    "    headcount_prn_by_role = headcount_filtered_prn.groupby('Standardized_Job_Title').size().rename('Headcount_PRN')\n",
    "    \n",
    "    # Hires by job title\n",
    "    hires_by_role = hires_filtered.groupby('Standardized_Job_Title').size().rename('Hires_All')\n",
    "    hires_fte_by_role = hires_filtered_fte.groupby('Standardized_Job_Title').size().rename('Hires_FTE')\n",
    "    hires_prn_by_role = hires_filtered_prn.groupby('Standardized_Job_Title').size().rename('Hires_PRN')\n",
    "    \n",
    "    # Terminations by job title\n",
    "    terminations_by_role = terminations_filtered.groupby('Standardized_Job_Title').size().rename('Terminations_All')\n",
    "    terminations_fte_by_role = terminations_filtered_fte.groupby('Standardized_Job_Title').size().rename('Terminations_FTE')\n",
    "    terminations_prn_by_role = terminations_filtered_prn.groupby('Standardized_Job_Title').size().rename('Terminations_PRN')\n",
    "    \n",
    "    # Requisitions by job title\n",
    "    if 'Req_Status' in requisitions_filtered.columns:\n",
    "        open_statuses = ANALYSIS_CONFIG['open_req_statuses']\n",
    "        closed_statuses = ANALYSIS_CONFIG['closed_req_statuses']\n",
    "        \n",
    "        requisitions_by_{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Healthcare Workforce Analysis Template\n",
    "\n",
    "This template provides a standardized framework for analyzing healthcare workforce data across multiple health systems. It emphasizes validation, transparency, and analyst control.\n",
    "\n",
    "## Design Principles:\n",
    "- **No silent failures** - every step validates its work\n",
    "- **User controls** all mappings and parameters\n",
    "- **Consistent variable naming** for downstream use\n",
    "- **Heavy validation** with hard stops when data quality is insufficient\n",
    "- **Modular sections** that can be validated step-by-step\n",
    "\n",
    "## Workflow Sections:\n",
    "1. Data Import & Schema Setup\n",
    "2. Initial Data Exploration & Validation  \n",
    "3. Geographic Filtering\n",
    "4. Column Standardization & Mapping\n",
    "5. Job Title Standardization\n",
    "6. Employment Status Standardization\n",
    "7. Location/Facility Standardization & Classification\n",
    "8. Core Metrics Calculation\n",
    "9. Facility-Level Analysis Setup\n",
    "10. Output Generation Framework\n",
    "11. Final Validation & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from math import gcd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Data Import & Schema Setup\n",
    "\n",
    "**🔧 USER CONFIGURATION REQUIRED** \n",
    "\n",
    "Modify the configuration below to match your data sources and column structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SECTION 1: DATA IMPORT & SCHEMA SETUP\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# USER CONFIGURATION - MODIFY THESE SETTINGS\n",
    "# ==========================================\n",
    "\n",
    "# File paths and data sources\n",
    "DATA_CONFIG = {\n",
    "    'base_folder': '/path/to/your/data',  # Base folder containing data files\n",
    "    'headcount_source': {\n",
    "        'file': 'System2024.xlsx',\n",
    "        'sheet': 'Headcount',\n",
    "        'type': 'excel'  # 'excel' or 'csv'\n",
    "    },\n",
    "    'hires_source': {\n",
    "        'file': 'System2024.xlsx', \n",
    "        'sheet': 'Hires',\n",
    "        'type': 'excel'\n",
    "    },\n",
    "    'terminations_source': {\n",
    "        'file': 'System2024.xlsx',\n",
    "        'sheet': 'Terms', \n",
    "        'type': 'excel'\n",
    "    },\n",
    "    'requisitions_source': {\n",
    "        'file': 'System2024.xlsx',\n",
    "        'sheet': 'Requisitions',\n",
    "        'type': 'excel'\n",
    "    },\n",
    "    'contractors_source': {\n",
    "        'file': 'System2024.xlsx',\n",
    "        'sheet': 'Contractor',\n",
    "        'type': 'excel'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Column mapping - USER MUST SPECIFY THESE\n",
    "COLUMN_MAPPING = {\n",
    "    'job_title_col': 'Job Title',\n",
    "    'location_col': 'Location', \n",
    "    'state_col': 'Work State',\n",
    "    'full_part_time_col': 'Full/Part Time',\n",
    "    'employment_category_col': 'Employment Category',\n",
    "    'hire_date_col': 'Hire Date',  # For hires sheet\n",
    "    'term_date_col': 'Termination Date',  # For terminations sheet\n",
    "    'req_status_col': 'Req Current State',  # For requisitions sheet\n",
    "}\n",
    "\n",
    "# Analysis parameters\n",
    "ANALYSIS_CONFIG = {\n",
    "    'target_state': 'NC',  # State to filter for\n",
    "    'analysis_date': '2024-12-31',  # Date for analysis\n",
    "    'target_job_titles': ['Registered Nurse', 'Licensed Practical Nurse', 'Medical Assistant', 'Certified Nursing Assistant'],\n",
    "    'fte_values': ['FULL_TIME'],  # Values that indicate FTE status\n",
    "    'prn_values': ['PRN'],  # Values that indicate PRN status\n",
    "    'open_req_statuses': ['Not Posted', 'Posted', 'Unposted', 'In Progress'],\n",
    "    'closed_req_statuses': ['Suspended', 'Expired']\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"Target state: {ANALYSIS_CONFIG['target_state']}\")\n",
    "print(f\"Analysis date: {ANALYSIS_CONFIG['analysis_date']}\")\n",
    "print(f\"Target job titles: {ANALYSIS_CONFIG['target_job_titles']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(source_config, dataset_name):\n",
    "    \"\"\"Load a dataset based on configuration\"\"\"\n",
    "    try:\n",
    "        filepath = f\"{DATA_CONFIG['base_folder']}/{source_config['file']}\"\n",
    "        \n",
    "        if source_config['type'] == 'excel':\n",
    "            df = pd.read_excel(filepath, sheet_name=source_config['sheet'])\n",
    "            print(f\"✓ Loaded {dataset_name}: {len(df)} rows from {source_config['file']}/{source_config['sheet']}\")\n",
    "        elif source_config['type'] == 'csv':\n",
    "            df = pd.read_csv(filepath)\n",
    "            print(f\"✓ Loaded {dataset_name}: {len(df)} rows from {source_config['file']}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {source_config['type']}\")\n",
    "            \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ CRITICAL ERROR loading {dataset_name}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Load all datasets\n",
    "print(\"\\nLoading datasets...\")\n",
    "try:\n",
    "    headcount_df = load_dataset(DATA_CONFIG['headcount_source'], 'headcount')\n",
    "    hires_df = load_dataset(DATA_CONFIG['hires_source'], 'hires')\n",
    "    terminations_df = load_dataset(DATA_CONFIG['terminations_source'], 'terminations')\n",
    "    requisitions_df = load_dataset(DATA_CONFIG['requisitions_source'], 'requisitions')\n",
    "    \n",
    "    # Contractors is optional\n",
    "    try:\n",
    "        contractors_df = load_dataset(DATA_CONFIG['contractors_source'], 'contractors')\n",
    "        has_contractors = True\n",
    "    except:\n",
    "        print(\"! No contractors data found - proceeding without it\")\n",
    "        contractors_df = pd.DataFrame()\n",
    "        has_contractors = False\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ CRITICAL: Data loading failed. Cannot proceed.\")\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\n✓ Data loading complete. {4 + int(has_contractors)} datasets loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Initial Data Exploration & Validation\n",
    "\n",
    "Validate dataset structure and show data previews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 2: INITIAL DATA EXPLORATION & VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def validate_dataset_structure(df, dataset_name, required_columns):\n",
    "    \"\"\"Validate that dataset has required structure\"\"\"\n",
    "    print(f\"\\nValidating {dataset_name} structure:\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "    \n",
    "    missing_cols = []\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            missing_cols.append(col)\n",
    "            \n",
    "    if missing_cols:\n",
    "        print(f\"✗ CRITICAL: Missing required columns in {dataset_name}: {missing_cols}\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"✓ All required columns present in {dataset_name}\")\n",
    "        return True\n",
    "\n",
    "# Define required columns for each dataset\n",
    "REQUIRED_COLUMNS = {\n",
    "    'headcount': [COLUMN_MAPPING['job_title_col'], COLUMN_MAPPING['location_col'], \n",
    "                  COLUMN_MAPPING['state_col'], COLUMN_MAPPING['full_part_time_col'], \n",
    "                  COLUMN_MAPPING['employment_category_col']],\n",
    "    'hires': [COLUMN_MAPPING['job_title_col'], COLUMN_MAPPING['location_col'], \n",
    "              COLUMN_MAPPING['state_col'], COLUMN_MAPPING['hire_date_col']],\n",
    "    'terminations': [COLUMN_MAPPING['job_title_col'], COLUMN_MAPPING['location_col'], \n",
    "                     COLUMN_MAPPING['state_col'], COLUMN_MAPPING['term_date_col']],\n",
    "    'requisitions': [COLUMN_MAPPING['job_title_col'], COLUMN_MAPPING['location_col'], \n",
    "                     COLUMN_MAPPING['req_status_col']]\n",
    "}\n",
    "\n",
    "# Validate all datasets\n",
    "validation_passed = True\n",
    "datasets_to_validate = [\n",
    "    (headcount_df, 'headcount'),\n",
    "    (hires_df, 'hires'), \n",
    "    (terminations_df, 'terminations'),\n",
    "    (requisitions_df, 'requisitions')\n",
    "]\n",
    "\n",
    "if has_contractors:\n",
    "    datasets_to_validate.append((contractors_df, 'contractors'))\n",
    "\n",
    "for df, name in datasets_to_validate:\n",
    "    if name in REQUIRED_COLUMNS:\n",
    "        if not validate_dataset_structure(df, name, REQUIRED_COLUMNS[name]):\n",
    "            validation_passed = False\n",
    "\n",
    "if not validation_passed:\n",
    "    print(\"\\n✗ CRITICAL: Dataset validation failed. Check column mappings and try again.\")\n",
    "    raise ValueError(\"Dataset validation failed\")\n",
    "\n",
    "print(\"\\n✓ All datasets passed structure validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data previews\n",
    "print(\"\\nData previews:\")\n",
    "for df, name in datasets_to_validate:\n",
    "    if len(df) > 0:\n",
    "        print(f\"\\n{name.upper()} - First 3 rows:\")\n",
    "        display(df.head(3))\n",
    "        \n",
    "        # Show unique values for key columns\n",
    "        if COLUMN_MAPPING['state_col'] in df.columns:\n",
    "            states = df[COLUMN_MAPPING['state_col']].unique()\n",
    "            print(f\"  Unique states: {states}\")\n",
    "            \n",
    "        if COLUMN_MAPPING['job_title_col'] in df.columns:\n",
    "            job_titles = df[COLUMN_MAPPING['job_title_col']].value_counts().head(5)\n",
    "            print(f\"  Top 5 job titles: {dict(job_titles)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Geographic Filtering\n",
    "\n",
    "Filter all datasets for the target state and validate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 3: GEOGRAPHIC FILTERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def filter_by_state(df, dataset_name, state_col, target_state):\n",
    "    \"\"\"Filter dataset by state and validate results\"\"\"\n",
    "    original_count = len(df)\n",
    "    \n",
    "    if state_col not in df.columns:\n",
    "        print(f\"✗ CRITICAL: State column '{state_col}' not found in {dataset_name}\")\n",
    "        raise ValueError(f\"State column missing in {dataset_name}\")\n",
    "    \n",
    "    # Show unique states before filtering\n",
    "    unique_states = df[state_col].unique()\n",
    "    print(f\"\\n{dataset_name} - States before filtering: {unique_states}\")\n",
    "    \n",
    "    # Filter for target state\n",
    "    df_filtered = df[df[state_col] == target_state].copy()\n",
    "    filtered_count = len(df_filtered)\n",
    "    \n",
    "    print(f\"{dataset_name} filtering: {original_count} → {filtered_count} rows\")\n",
    "    \n",
    "    if filtered_count == 0:\n",
    "        print(f\"✗ CRITICAL: No data remains after filtering {dataset_name} for {target_state}\")\n",
    "        raise ValueError(f\"No {target_state} data in {dataset_name}\")\n",
    "    \n",
    "    # Validation - check that filtering worked\n",
    "    remaining_states = df_filtered[state_col].unique()\n",
    "    if len(remaining_states) != 1 or remaining_states[0] != target_state:\n",
    "        print(f\"✗ CRITICAL: Filtering failed for {dataset_name}. Remaining states: {remaining_states}\")\n",
    "        raise ValueError(f\"State filtering failed for {dataset_name}\")\n",
    "    \n",
    "    print(f\"✓ {dataset_name} successfully filtered to {target_state}\")\n",
    "    return df_filtered\n",
    "\n",
    "# Filter all datasets\n",
    "target_state = ANALYSIS_CONFIG['target_state']\n",
    "state_col = COLUMN_MAPPING['state_col']\n",
    "\n",
    "print(f\"Filtering all datasets for state: {target_state}\")\n",
    "\n",
    "headcount_df = filter_by_state(headcount_df, 'headcount', state_col, target_state)\n",
    "hires_df = filter_by_state(hires_df, 'hires', state_col, target_state)\n",
    "terminations_df = filter_by_state(terminations_df, 'terminations', state_col, target_state)\n",
    "requisitions_df = filter_by_state(requisitions_df, 'requisitions', state_col, target_state)\n",
    "\n",
    "if has_contractors:\n",
    "    contractors_df = filter_by_state(contractors_df, 'contractors', state_col, target_state)\n",
    "\n",
    "print(f\"\\n✓ Geographic filtering complete. All datasets filtered to {target_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Column Standardization & Mapping\n",
    "\n",
    "Apply column standardization to ensure consistent naming across datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 4: COLUMN STANDARDIZATION & MAPPING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def standardize_columns(df, dataset_name):\n",
    "    \"\"\"Apply column standardization to ensure consistent naming\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ensure all mapped columns exist and create standard names\n",
    "    column_renames = {}\n",
    "    \n",
    "    # Standard columns all datasets should have\n",
    "    if COLUMN_MAPPING['job_title_col'] in df.columns:\n",
    "        column_renames[COLUMN_MAPPING['job_title_col']] = 'Job_Title'\n",
    "    if COLUMN_MAPPING['location_col'] in df.columns:\n",
    "        column_renames[COLUMN_MAPPING['location_col']] = 'Location'\n",
    "    if COLUMN_MAPPING['state_col'] in df.columns:\n",
    "        column_renames[COLUMN_MAPPING['state_col']] = 'State'\n",
    "        \n",
    "    # Dataset-specific columns\n",
    "    if dataset_name in ['headcount', 'hires', 'terminations']:\n",
    "        if COLUMN_MAPPING['full_part_time_col'] in df.columns:\n",
    "            column_renames[COLUMN_MAPPING['full_part_time_col']] = 'Full_Part_Time'\n",
    "        if COLUMN_MAPPING['employment_category_col'] in df.columns:\n",
    "            column_renames[COLUMN_MAPPING['employment_category_col']] = 'Employment_Category'\n",
    "    \n",
    "    if dataset_name == 'hires' and COLUMN_MAPPING['hire_date_col'] in df.columns:\n",
    "        column_renames[COLUMN_MAPPING['hire_date_col']] = 'Date'\n",
    "    if dataset_name == 'terminations' and COLUMN_MAPPING['term_date_col'] in df.columns:\n",
    "        column_renames[COLUMN_MAPPING['term_date_col']] = 'Date'\n",
    "    if dataset_name == 'requisitions' and COLUMN_MAPPING['req_status_col'] in df.columns:\n",
    "        column_renames[COLUMN_MAPPING['req_status_col']] = 'Req_Status'\n",
    "    \n",
    "    # Apply renames\n",
    "    df = df.rename(columns=column_renames)\n",
    "    \n",
    "    print(f\"✓ {dataset_name} columns standardized. Renamed: {column_renames}\")\n",
    "    return df\n",
    "\n",
    "# Standardize all datasets\n",
    "print(\"Standardizing column names across all datasets...\")\n",
    "\n",
    "headcount_df = standardize_columns(headcount_df, 'headcount')\n",
    "hires_df = standardize_columns(hires_df, 'hires')\n",
    "terminations_df = standardize_columns(terminations_df, 'terminations')\n",
    "requisitions_df = standardize_columns(requisitions_df, 'requisitions')\n",
    "\n",
    "if has_contractors:\n",
    "    contractors_df = standardize_columns(contractors_df, 'contractors')\n",
    "\n",
    "print(\"✓ All datasets have standardized column names\")\n",
    "\n",
    "# Validation - ensure key columns are present\n",
    "for df, name in [(headcount_df, 'headcount'), (hires_df, 'hires'), \n",
    "                 (terminations_df, 'terminations'), (requisitions_df, 'requisitions')]:\n",
    "    if 'Job_Title' not in df.columns or 'Location' not in df.columns:\n",
    "        print(f\"✗ CRITICAL: Missing standardized columns in {name}\")\n",
    "        raise ValueError(f\"Column standardization failed for {name}\")\n",
    "\n",
    "print(\"✓ Column standardization validation passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Job Title Standardization\n",
    "\n",
    "Standardize job titles to target categories and filter for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SECTION 5: JOB TITLE STANDARDIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def standardize_job_title(title):\n",
    "    \"\"\"\n",
    "    Standardize job titles to target categories:\n",
    "    - Registered Nurse\n",
    "    - Licensed Practical Nurse  \n",
    "    - Medical Assistant\n",
    "    - Certified Nursing Assistant\n",
    "    \"\"\"\n",
    "    if pd.isna(title):\n",
    "        return None\n",
    "    \n",
    "    title_lower = str(title).lower().strip()\n",
    "    \n",
    "    # Registered Nurse variations\n",
    "    if 'rn' in title_lower or 'registered nurse' in title_lower:\n",
    "        # Exclude non-employee entries\n",
    "        if '(nonee)' in title_lower or 'traveler' in title_lower:\n",
    "            return None\n",
    "        return 'Registered Nurse'\n",
    "    \n",
    "    # Licensed Practical Nurse variations\n",
    "    elif 'lpn' in title_lower or 'licensed practical nurse' in title_lower:\n",
    "        return 'Licensed Practical Nurse'\n",
    "    \n",
    "    # Medical Assistant variations\n",
    "    elif 'medical assistant' in title_lower or 'certified medical assistant' in title_lower:\n",
    "        return 'Medical Assistant'\n",
    "    \n",
    "    # Certified Nursing Assistant variations\n",
    "    elif ('cna' in title_lower or 'certified nursing assistant' in title_lower or \n",
    "          'certified nurse assistant' in title_lower or 'nurse assistant' in title_lower or \n",
    "          'nursing assistant' in title_lower):\n",
    "        return 'Certified Nursing Assistant'\n",
    "    \n",
    "    # If none match, return None (will be filtered out)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_job_title_standardization(df, dataset_name):\n",
    "    \"\"\"Apply job title standardization and validate results\"\"\"\n",
    "    original_count = len(df)\n",
    "    \n",
    "    # Apply standardization\n",
    "    df = df.copy()\n",
    "    df['Standardized_Job_Title'] = df['Job_Title'].apply(standardize_job_title)\n",
    "    \n",
    "    # Filter for target roles only\n",
    "    df_filtered = df[df['Standardized_Job_Title'].notna()].copy()\n",
    "    filtered_count = len(df_filtered)\n",
    "    \n",
    "    # Calculate capture rate\n",
    "    capture_rate = (filtered_count / original_count * 100) if original_count > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{dataset_name} job title standardization:\")\n",
    "    print(f\"  Original rows: {original_count}\")\n",
    "    print(f\"  Rows with target job titles: {filtered_count}\")\n",
    "    print(f\"  Capture rate: {capture_rate:.1f}%\")\n",
    "    \n",
    "    # Show distribution of standardized titles\n",
    "    title_dist = df_filtered['Standardized_Job_Title'].value_counts()\n",
    "    print(f\"  Distribution: {dict(title_dist)}\")\n",
    "    \n",
    "    # Validation - ensure we have reasonable capture rate\n",
    "    if capture_rate < 10:\n",
    "        print(f\"✗ CRITICAL: Very low capture rate ({capture_rate:.1f}%) for {dataset_name}\")\n",
    "        print(\"This suggests job title standardization is not working properly.\")\n",
    "        # Show some examples of unmapped titles\n",
    "        unmapped = df[df['Standardized_Job_Title'].isna()]['Job_Title'].value_counts().head(10)\n",
    "        print(f\"Top unmapped titles: {dict(unmapped)}\")\n",
    "        raise ValueError(f\"Job title standardization failed for {dataset_name}\")\n",
    "    \n",
    "    if filtered_count == 0:\n",
    "        print(f\"✗ CRITICAL: No target job titles found in {dataset_name}\")\n",
    "        raise ValueError(f\"No target job titles in {dataset_name}\")\n",
    "    \n",
    "    print(f\"✓ {dataset_name} job title standardization successful\")\n",
    "    return df_filtered\n",
    "\n",
    "# Apply job title standardization to all datasets\n",
    "print(\"Applying job title standardization...\")\n",
    "print(f\"Target job titles: {ANALYSIS_CONFIG['target_job_titles']}\")\n",
    "\n",
    "headcount_filtered = apply_job_title_standardization(headcount_df, 'headcount')\n",
    "hires_filtered = apply_job_title_standardization(hires_df, 'hires')\n",
    "terminations_filtered = apply_job_title_standardization(terminations_df, 'terminations')\n",
    "requisitions_filtered = apply_job_title_standardization(requisitions_df, 'requisitions')\n",
    "\n",
    "if has_contractors:\n",
    "    contractors_filtered = apply_job_title_standardization(contractors_df, 'contractors')\n",
    "\n",
    "print(\"\\n✓ Job title standardization complete for all datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-dataset validation - ensure we have consistent job titles\n",
    "all_job_titles = set()\n",
    "for df, name in [(headcount_filtered, 'headcount'), (hires_filtered, 'hires'),\n",
    "                 (terminations_filtered, 'terminations'), (requisitions_filtered, 'requisitions')]:\n",
    "    titles = set(df['Standardized_Job_Title'].unique())\n",
    "    all_job_titles.update(titles)\n",
    "    \n",
    "expected_titles = set(ANALYSIS_CONFIG['target_job_titles'])
if not all_job_titles.issubset(expected_titles):
    print(f"✗ WARNING: Unexpected job titles found: {all_job_titles - expected_titles}")

print(f"✓ Job title validation complete. Found titles: {sorted(all_job_titles)}")